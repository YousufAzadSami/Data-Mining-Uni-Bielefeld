{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Applications/anaconda/lib/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    }
   ],
   "source": [
    "# settings...\n",
    "%pylab inline\n",
    "import scipy, scipy.stats\n",
    "pylab.rcParams['figure.figsize'] = (8.0, 4.0)\n",
    "from IPython.html.widgets import interact, fixed\n",
    "# import ipywidgets\n",
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\usepackage{amssymb} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\vx}{\\vec{x}} \\newcommand{\\vy}{\\vec{y}} \\newcommand{\\vw}{\\vec{w}} \\newcommand{\\Wc}{{\\bf W}_c}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Prototype-based Clustering\n",
    "\n",
    "Goal: Partitioning by assignment of $N$ data points to $K<N$ typical prototypes (representing the clusters)\n",
    "\n",
    "There are two variants of the assignment:\n",
    "\n",
    "(1.) **Deterministic Assignment**: \n",
    " * Each data point is exactly assigned to a single prototype (hard clustering)\n",
    " \n",
    "\n",
    "(2.) **Probabilistic Assignment**: \n",
    " * Each data point $\\vx_i$  is assigned with probability $h_{ij}$ to the $j$-th prototype (soft clustering)\n",
    "$$\n",
    "\t\\sum_{j=1}^K h_{ij} = 1 \\forall~i ~ ~ ~\\text{(normalization)}\n",
    "$$\n",
    "\n",
    "** Remark: **\n",
    "vectors\n",
    "* The assignment (or membership) matrix $H$ has N=nr of data points rows and K=nr. of prototypes columns. \n",
    " * each row sums to 1 (i.e. each data point is assigned)\n",
    " * each column k sums to the net mass of the kth cluster\n",
    "* (1.) is a special case of (2.) with $h_{i*}$ being the canonical basis  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5.1. Hard Clustering by Vector Quantization\n",
    "\n",
    "Idea: Each data point $\\vx_i$ is assigned to one of $K$ prototypes represented by $\\{\\vec{w}_j\\}_{j=1\\dots K}$ by using a label $l(\\vx_i)\\in\\{1,\\dots,K\\}$\n",
    "* This can be regarded as a compression for lossy transmission.\n",
    " 1. Coding: $\\vec{x} \\to l \\in \\{1,\\dots, K\\}$\n",
    " 2. Decoding: $l \\to \\vec{w}_l$\n",
    "\n",
    "* Goal: Minimization of the average quantization errors $E$ by varying the $\\vw_j$.\n",
    "\n",
    "* Possible Error function\n",
    "$$\n",
    "E = \\frac{1}{N} \\sum\\limits_{i=1}^N\\sum\\limits_{j=1}^K h_{ij} \\| \\vx_i - \\vw_j\\|^2\n",
    "$$\n",
    "with $h_{ij} = \\left\\{  \\begin{array}{ll} 1 & j = l(x_i) \\\\  0 & \\text{else} \\\\      \\end{array} \\right.$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Gradient descent is not possible because the assignment variables $h_{ij}$ are discrete.\n",
    "\n",
    "The following algorithm allows optimization:\n",
    "1. Initialization of prototypes, e.g. $\\vec{w}_j = \\vx_j,~j=1\\dots K$\n",
    "2. While keeping prototypes fixed, choose assignment $h_{ij}$ so that $E$ is minimized (Voronoi cells, winnter-takes all rule)\n",
    "3. While keeping assignments fixed, choose prototypes so that $E$ is minimized, i.e. $\\nabla_{\\vec{w}} E = 0$ \n",
    "\\begin{eqnarray}\n",
    "\\nabla_{\\vw_j} E &=& \\frac{1}{N} \\sum\\limits_{i=1}^N h_{ij}(2\\vw_j - 2\\vx_i) \\stackrel{!}{=} 0\\\\\n",
    "\\Rightarrow \\vw_j &=& \\frac{\\sum_i h_{ij} \\vx_{i}}{\\sum\\limits_i h_{ij}} ~ ~ \\forall ~ j\n",
    "\\end{eqnarray}\n",
    " * Note that the denominator counts all data points with the $j$th voronoi cell\n",
    " * The solution is analog to the 'principal points' before, here for each cluster\n",
    "4. If the error decrease $|\\Delta E | < \\epsilon$: stop, else goto 2.\n",
    "\n",
    "Remarks:\n",
    "* Convergence to local minima is certain since step 2 and 3 reduce the error and $E \\ge 0$ is a lower limit.\n",
    "* The result depends on the initialization. Due to the risk of getting stuck in suboptimal local minima, usually different initializations are tested and the best clustering taken.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5.2. Soft clustering with Mixture Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Implementing the possibility to represent uncertainty in the assignment of data to clusters (expressed in form of assignment probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Approach:**\n",
    "* Description of the data distribution by a probability density $p(\\vx)$.\n",
    "* Instead of disjunct partitioning into 'hard' clusters $\\to$ mixture model:\n",
    " * decomposition of $p(\\vx)$ in additive cluster terms which may overlap and thus describe a soft cluster partitioning\n",
    "$$\n",
    "\tp(\\vx) = \\sum_{j=1}^M g_j {\\cal N}(\\vx, \\mu_j, \\Sigma_j)\\\\\n",
    "\t\\mbox{with normalization} \\sum\\limits_{j=1}^M g_j =1 \n",
    "$$\n",
    "* $N$ is the normal distribution, but any other distribution is also possible"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Interpretation**:\n",
    "* Each summand describes a soft cluster $C_j$ with centroid $\\vec{\\mu}_j$ and a spatial extension which is determined by the covariance matrix  $\\Sigma_j$ \n",
    "* The parameter $g_j=p(j)$ represents the probability at which an arbitrary data point belongs to the $j$-th cluster, i.e. the a priori probability for the membership to a cluster\n",
    "* Knowledge of a data point $\\vx$ allows the computation of a more precise a posteriori membership probability to cluster $j$:\n",
    "$$\n",
    "\tp_j(\\vx)= \\frac{g_j N(\\vx,\\vec{\\mu}_j,\\Sigma_j)}\n",
    "\t\t{\\sum\\limits_k g_k N(\\vx, \\vec{\\mu}_k,\\Sigma_k)} \n",
    "$$\n",
    " * according to the Bayes's theorem, here with $ p(\\vx | j)=N(\\vec{\\mu}_j, \\Sigma)$ \n",
    "* Note that the probabilitites $p_j(\\vx_i)$ replace the binary cluster memberships $h_{ij}$ used in hard clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Question**:\n",
    "What parameters $\\Theta = \\{ g_j,\\vec{\\mu}_j,\\Sigma_j\\}$ are the most probable ones according to the information given in the data set $D$?\n",
    "$$\n",
    "\tP(\\Theta| \\underbrace{\\{ \\vx_1 \\dots \\vx_n\\}}_{=D})= \n",
    "\t\t\\frac{P(D|\\Theta) P(\\Theta)}{\\underbrace{\\sum\\limits_\\Theta P(D|\\Theta) P(\\Theta)}_{=P(D)}}\n",
    "$$\n",
    "* the prior distribution for $\\Theta$ without prejudice: $\\Rightarrow P(\\Theta)=1$\n",
    "* Likelihood function $L$: is the $\\Theta$-dependent part of the right side of the above equation.\n",
    "\\begin{eqnarray}\n",
    "\t{\\cal L} = \\log L = \\log P(D|\\Theta) &=& \\log \\prod\\limits_{i=1}^N p(\\vx_i)\\\\\n",
    "\t{\\cal L}[\\{\\vec{g},\\vec{\\mu},\\Sigma\\}]= \\sum\\limits_{i=1}^N \\log p(\\vx_i) &=&\n",
    "\t\\sum\\limits_{i=1}^N \\log \\left [ \\sum\\limits_{k=1}^K g_k N(\\vx_i,\\vec{\\mu}_k,\\Sigma_k) \\right ]\n",
    "\\end{eqnarray}\n",
    " * Note that this is a product since the data points are independent samples from the distribution.\n",
    "* we seek the maximum of L regarding $g_k,\\vec{\\mu}_k,\\Sigma_k$!\n",
    "* side condition: $\\sum\\limits_{j=1}^K g_j = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be shown that a local maximization of $\\cal{L}$ can be achieved by the following heuristic motivated iterative method:\n",
    "* the trick is the application of Jensen's inequality $\\ln(\\sum_j \\lambda_j q_j) \\ge \\sum_j \\lambda_j \\ln(q_j)$ to simplify the sum, e.g. see Bishop, Neural Networks for Pattern Recognition, Mixture Models, S. 67, or: http://en.wikipedia.org/wiki/Expectation-maximization_algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Initialization: Starting values $t=0,~g_k^0,~\\vec{\\mu}_k^0,~\\Sigma_k$\n",
    " * e.g. obtained by one of the otehr cluster methods\n",
    "2. Computation of new membership probabilities (E-Step):\n",
    "$$\n",
    "p_j^{t+1}(\\vec{x}_i) = \\frac{g^t_j~ {\\cal N}(\\vec{x}_i;\\mu^t_j,\\Sigma^t_j)}\n",
    "\t{\\sum\\limits_k g^t_k{\\cal N}(\\vec{x}_i;\\mu^t_k,\\Sigma^t_k)} ~ ~ ~ ~ ~ ~ ~ \\text{corresponds to} ~ h_{ij}^{(t+1)}\n",
    "$$\n",
    " * $p_j(\\vx_i)$ corresponds to the (here now continuous) $h_{ij}$ from hard clustering\n",
    "3. Computation of new estimates for the cluster centers $\\vec{\\mu}^{t+1}_k$ as well as for the cluster covariances $\\Sigma^{t+1}_k$:\n",
    "\\begin{eqnarray}\n",
    "\tg_k^{t+1} &=& \\frac{1}{N}\\sum\\limits_i p_k^{t+1}(\\vec{x}_i) \\\\\n",
    "\t          & & \\hbox{(updated probability mass in cluster $k$)} \\\\\n",
    "\t\\vec{\\mu}_k^{t+1} &=& \\frac{1}{g^{t+1}_k}\\sum\\limits_i p_k^{t+1}(\\vec{x}_i)\\cdot\\vec{x_i}\\\\\n",
    "    & & \\hbox{(updated centroid of cluster $k$)} \\\\\n",
    "\t\\Sigma_k^{t+1} &=&  \\frac{1}{g^{t+1}_k}\\sum\\limits_i \\left( p_k^{t+1}(\\vec{x}_i)\\cdot \n",
    "\t(\\vec{x}_i-\\vec{\\mu}_k)(\\vec{x}_i-\\vec{\\mu}_k)^T \\right)\\\\\n",
    "    & & \\hbox{(updated covariance of cluster $k$)} \\\\\n",
    "\\end{eqnarray}\n",
    "4. If the maximal number of iterations has not yet been reached: goto 2.\n",
    "\n",
    "This method is known as EM algorihtm (Expectation-Maximization algorithm)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**:\n",
    "* EM-algorithm delivers local optima and no garantuee for finding a global optimum \n",
    "* remedy: repeat with different initializations, take the best result.\n",
    "* Convergence problems with 'degenerated clusters', i.e. if $\\Sigma_k$ is almost singular\n",
    " * this can happen if too few data points $<d$ are within a cluster\n",
    " * or is all points lie within a linear subspace\n",
    "* here: special solution under the assumption of normal distributed clusters and vectorial represented data.\n",
    "\n",
    "* The results of the clustering is similar to the Ward-clustering, but now the membership is not greedy, but globally and thus more effort, but potentially better\n",
    "* Attention: minimization makes only sense for a given number of clusters $c$. Optimizing additionally w.r.t. $c$ is useless: this leads always to the trivial solution of $c=N$ clusters centered at data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example (from wikipedia)\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/6/69/EM_Clustering_of_Old_Faithful_data.gif\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6. Evaluation of Clustering results\n",
    "       \n",
    "Questions:\n",
    "* How many clusters exist?\n",
    "* How unique is the found clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.1. The number of clusters\n",
    "\n",
    "** Basic Idea**: \n",
    "* Compare the cluster quality for a given number of clusters $c$\n",
    "* choose the smallest number of clusters with acceptable quality.\n",
    "* $\\to$ this shifts the problem towards the definition of a suitable cluster quality measure.\n",
    "\n",
    "** Gap-Statistic ** (Tibshirani, G. Walther, T. Hastie, 2001, <https://web.stanford.edu/~hastie/Papers/gap.pdf>):\n",
    "* compares the logarithmized intra cluster variance\n",
    "$$\n",
    "\tg_c(D) = \\log \\hbox{Tr}(\\Wc(D)) \n",
    "$$\n",
    "for $c$ clusters with their centroid $\\langle g_c(U_i)\\rangle_i$ \n",
    "for enforced clustering of $M$ uniform distributed random sets $U_i$ $(i=1,\\ldots M)$ into likewise $c$ clusters.\n",
    "* original formulation:\n",
    "    $$ = \\log \\sum_{r=1}^c \\frac{1}{2n_r} D_r$$ \n",
    " * with $n_r = |C_r|$ (size of clusters)\n",
    " * and $D_r = \\sum\\limits_{i,i' \\in C_r} d(\\vec{x}_i, \\vec{x}_{i'})$ (data point distances) \n",
    " * interestingly $D_r$ is $2E$, two times the MSE to cluster centers, if d is the squared Euclidean distance\n",
    "* the first 'clear/distinct' peak of the difference \n",
    "$$\t\\hbox{Gap}(c) = \\langle g_c(U_i)\\rangle - g_c(D) $$\n",
    "points at the 'true' number of clusters $c_{opt}$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Example illustration from Tibshirani:\n",
    "<img src=\"images/Tibshirani_gap.png\" width=\"75%\">\n",
    " * Figure (b) depicts the gap for the data set in Figure (a)\n",
    "* more accurate is $c_{opt} = $ smallest value $c$, for which \n",
    "$$\t\\hbox{Gap}(c) \\ge \\hbox{Gap}(c+1) - s_{c+1} $$\n",
    "where \n",
    "$$\ts_{c} = \\sqrt{\\left(1+\\frac{1}{M}\\right) \\hbox{Var}(g_c(U_i))} $$\n",
    "* The $U_i$ are randomly generated as equally large  ($\\vert U_i\\vert=\\vert D\\vert$) random sets of a $D$-containing box in feature space.\n",
    "* If the data distribution is longer than thick, it is recommended to align the box along the principal axes of the data distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Further, more simple propositions **:\n",
    "\n",
    "* Approach by Calinsky & Harabasz:\n",
    "$$\n",
    "\tc_{opt} = \\arg\\max\\limits_{c\\ge 2}\\frac{n-c}{c-1}\\frac{\\hbox{trace}({\\bf B}_c)}{\\hbox{trace}(\\Wc)}\n",
    "$$\n",
    " * Only applicable for $c\\ge 2$\n",
    " * performed best in a comparison of 20 alternative approaches\n",
    "\n",
    "* Alternative approach by Hartican (1975):\n",
    "$$\tc_{opt} = \\min\\limits_c\\left \\{ c \\vert \\left(\\frac{\\hbox{trace}(\\Wc)}{\\hbox{trace}{(\\bf W}_{c+1})}-1\\right)\\ge 10(n-c-1)\\right\\}\n",
    "$$\n",
    " * The idea is to start with 1 cluster and add more as long as \n",
    " $H(c) = \\left(\\frac{\\hbox{trace}(\\Wc)}{\\hbox{trace}{(\\bf W}_{c+1})}-1\\right)\\frac{1}{n-c-1}$ is sufficiently large.\n",
    " \n",
    "* For mixture models the Bayes-Criterion is applicable:\n",
    "$$\n",
    "\\mbox{BIC}(c) = 2\\log P(D\\vert \\theta_c) - m_c \\log(n)\n",
    "$$\n",
    "    * First term: Log-Likelihood of the data $D$ at model parameters $\\theta_c$.\n",
    "    * Second term: penalty for models with large number of parameters $m_c$.\n",
    "    * $c_{opt}= $ value of $c$ at the first distinct maximum of BIC($\\cdot$).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 4.6.2. Uniqueness of Clustering results\n",
    "\n",
    "* Basic Idea: Compare results of repeated clustering runs under slight variations (see below)\n",
    "* Measure for comparison for two clusterings $C_1$ and $C_2$: \n",
    "\n",
    "**Rand-Index**\n",
    "\n",
    "$$ R_g = \\frac{N_{++} + N_{--}}\n",
    "\t\t\t{N_{++} + N_{--} + N_{+-} + N_{-+}} = \\frac{2 (N_{++} + N_{--})}{N(N-1)}\n",
    "$$\n",
    "with the following definitions:\n",
    "* $N_{++}$ is the number of data point pairs that are in the same cluster in $C_1$ and also in the same cluster in $C_2$ \n",
    "* $N_{+-}$ is the number of data point pairs that are in the same cluster in $C_1$ but in different clusters in $C_2$ \n",
    "* $N_{-+}$ is the number of data point pairs that are in different clusters in $C_1$ but in the same cluster in $C_2$ \n",
    "* $N_{--}$ is the number of data point pairs that are in different clusters in $C_1$ and also in different clusters in $C_2$ \n",
    "\n",
    "* The denominator is the number of all possible point pairs, i.e.  ${N \\choose 2} = N(N-1)/2$\n",
    "* $R_g=1 \\Rightarrow$ both clusterings are \"`the same\"'.\n",
    "\n",
    "* Possible modes of variation for the examination of clustering stability\n",
    " * different initializations\n",
    " * Repetition of the method on different data subsets (equal size)\n",
    " * ditto, but variable large data set size\n",
    " * elimination of few features should affect the result not too strongly\n",
    " * using new variables"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
