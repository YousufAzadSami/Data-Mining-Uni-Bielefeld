{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## settings \n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy, scipy.stats\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\usepackage{amssymb} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\vx}{\\vec{x}} \\newcommand{\\vy}{\\vec{y}} \\newcommand{\\vw}{\\vec{w}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4. Optimization methods for Clustering\n",
    "\n",
    "* Idea: derive clustering from an optimization (minimization) of a suitable cost function for clustering into $K$ clusters.\n",
    "* Success depends on the definition of a global optimality criterion\n",
    "\n",
    "* The usually applied criteria are derived from the covariance matrix of the data (resp. of the cluster or cluster centers).\n",
    "\n",
    "### Global data covariance matrix\n",
    "$$\n",
    "C = \\frac{1}{N} \\sum\\limits_{i=1}^K \\sum\\limits_{j=1}^{N_i} (\\vx_{ij}-\\vec{m})(\\vx_{ij}-\\vec{m})^\\tau\n",
    "$$\n",
    "where $N = \\sum\\limits_{i=1}^K N_i$ und $\\vec{m} = \\langle \\vx \\rangle$ ist.\n",
    "\n",
    "### Local covariance matrix (of any cluster $i$)\n",
    "$$\n",
    "C_i = \\frac{1}{N_i} \\sum\\limits_{j=1}^{N_i} (\\vx_{ij}-\\vec{m}_i)(\\vx_{ij}-\\vec{m}_i)^\\tau\n",
    "$$\n",
    "\n",
    "### Mean covariance matrix of all clusters\n",
    "$$\n",
    "W = \\frac{1}{N} \\sum\\limits_{i=1}^K N_i C_i\n",
    "$$\n",
    "* also called _within_-Variance\n",
    "\n",
    "### Covariance matrix of the cluster centers \n",
    "\n",
    "$$\n",
    "B = \\frac{1}{K} \\sum\\limits_{i=1}^K (\\vec{m}_i-\\vec{m})(\\vec{m}_i-\\vec{m})^\\tau\n",
    "$$\n",
    "also called _in between_-Variance.\n",
    "\n",
    "* Note that the origin is here the centroid $\\vec{m}$ of the data set, and not the centroid of the cluster centroid vectors: the clusters could contain differntly many data points.\n",
    "* matrices $B, C, W$ are not completely independent, but coupled via $C = B + c\\cdot W$, where $c$ is a real-valued factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality criteria (resp. cost functions)\n",
    "\n",
    "#### Minimization of trace$(W)$\n",
    "$$\n",
    "\\text{trace}(W) = \\text{trace}\\left( \\frac{1}{N} \\sum\\limits_{i,j}^{K, N_i}  (\\vx_{ij}-\\vec{m}_i)(\\vx_{ij}-\\vec{m}_i)^\\tau\\right)\n",
    "= \\sum_{i=1}^d \\lambda_i(W)\n",
    "$$\n",
    "where $\\lambda_i$ is the $i$-th eigenvalue of $W$.\n",
    "\n",
    "* Interpretation: trace$(W)$ is a measure for the total variance of a typical cluster, i.e. minimization corresponds to the claim for clusters with low total variance, i.e. spatial extension.\n",
    "* $\\Rightarrow$ this favours compact, rather spherical clusters.\n",
    "\n",
    "#### Minimization of the determinant  $\\det(W)$\n",
    "It is \n",
    "$$\n",
    "\\det(W) = \\prod_{i=1}^d \\lambda_i(W)\n",
    "$$\n",
    "Interpretation:\n",
    "* $\\sqrt{\\lambda_i}$ measures the mean extension of a typical cluster along the $i$th-eigenvector of $W$.\n",
    " * The determinant is thus a measure for the squared volume of a typical cluster.\n",
    "* Minimizing $\\det(W)$ corresponds to the demand of clusters with least volume\n",
    " * this may favour longish slim clusters\n",
    "* the quality function favours partitioning into cluster of similar form (incl. orientation!)\n",
    "* Since $C$ is const, one can as well maximize $\\det(C)/\\det(W)$ \n",
    " * Advantage: better normalization since this is a dimensionfree number.\n",
    "\n",
    "#### Maximization of $\\text{trace}(BW^{-1})$\n",
    "Interpretation: \n",
    "* In the 1D-case, one can understand that this favours simultaneously large variance of the cluster centroids and low dispersion within the clusters.\n",
    " * $W$ is in the denominator\n",
    "* Generalization to the high-dimensional case results in the matrix form of the given quality function.\n",
    "\n",
    "---\n",
    "\n",
    "* All previous optimization methods require the search of a maximal (resp. minimal) value of the quality functions $E$.\n",
    "* This can for instance be practically implemented by a stochastic search method of Simulated Annealing with $E$ as cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4.1 Simulated Annealing for Clustering\n",
    "** Basic Idea: **\n",
    "<img src=\"http://apmonitor.com/me575/uploads/Main/sim_annealing_flow.png\">\n",
    "Figure from http://apmonitor.com/me575/uploads/Main/sim_annealing_flow.png\n",
    "\n",
    "Basic Principle:\n",
    "1. random partitioning of the data set into initial clusters (e.g. heuristically)\n",
    "2. compute $\\Delta E_{ij}$ for each possible relabeling of an individual data element $x_i$ into cluster $C_j$.\n",
    "3. If $\\Delta E^{\\star} = \\min_{i,j} \\Delta E_{ij} < 0 $, execute the according relabeling.\n",
    " * else execute the relabeling only at probability $p = \\exp(-\\Delta E^{\\star}/T)$ (Boltzmann probability factor)\n",
    " * Note that $T$ is called temperature and expresses the stochasticity of the process\n",
    "4. Decrease $T$ slightly, e.g. by setting $T=\\alpha\\cdot T$ with $\\alpha$ close to but smaller than 1.\n",
    "5. Interruption Condition: after a fixed number of steps (the more the better)\n",
    "\n",
    "Remarks:\n",
    "* The temperature $T$ is slowly decreased, thus annealing.\n",
    "* Alternative to the temperature, the notation $\\beta=1/T$ is used, where $\\beta$ is increased for annealing.\n",
    "* at later time points, the acceptance is more and more limited to relabeling steps that improve the quality.\n",
    "* The acceptance of steps that worsens the quality reduces the risk of getting stuck in clusterings that are only optimal under small local changes.\n",
    "* The method depends on the initialization. The remedy is to repeat the algorithm with different initializations and use only the best clustering as result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an incremental calculation $W^{\\text{neu}} = W^{\\text{alt}} + \\Delta W$\n",
    "\n",
    "* After relabeling a data point $\\vec{x}_r$ from cluster $o$ into a new cluster $i$\n",
    "requires because of $C = const \\cdot W + B$ only to compute $\\Delta W$.\n",
    "* Regard the change of terms of $W$:\n",
    "$$ \\vec{m}_o \\leftarrow \\frac{n_o \\vec{m}_o-\\vx_r}{n_o - 1} \\quad\\quad \\Delta  n_o = -1$$\n",
    "$$\n",
    " \\vec{m}_i  \\leftarrow \\frac{n_i \\vec{m}_i+\\vx_r}{n_i +1} \\quad\\quad \\Delta  n_i = +1$$\n",
    "Thus we have: \n",
    "$$ W = \\frac{1}{N}\\sum_i N_i W_i$$\n",
    "with\n",
    "$$ W_i=\\frac{1}{N_i}\\sum^{n_i}_{j=1} (x_{ij}-\\vec{m}_i)(x_{ij}-\\vec{m}_i)^T$$\n",
    "    \n",
    "and with that:\n",
    "$$\\Delta W_i = (\\vx_r - \\vec{m}_i)(\\vx_r - \\vec{m}_i)^T \\frac{n_i}{n_i +1}$$\n",
    "$$\\Delta W_o = (\\vx_r - \\vec{m}_o)(\\vx_r - \\vec{m}_o)^T \\frac{n_o}{n_o -1}$$\n",
    "* in addition, the shift of the means needs to be corrected..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
