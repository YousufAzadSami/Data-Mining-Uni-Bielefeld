{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "# settings\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy, scipy.stats\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\usepackage{amssymb} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\vx}{\\vec{x}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4. Principal Component Analysis (PCA)\n",
    "\n",
    "* The PCA is a simple standard method for dimensionality reduction\n",
    "* Principal Idea: Determine a low-dimensional linear subspace that contains the largest share of the variance.\n",
    "\n",
    "<img src = \"images/projection.png\" width=\"30%\">\n",
    "\n",
    "**Given**: data set $ X = \\{\\vec{x}^\\alpha\\}_{\\alpha=1\\dots N}, \\vec{x}^\\alpha \\in \\R^d $\n",
    "Let w.l.o.g. the data be **centered**, i.e. all features are shifted/translated so that means are 0:\n",
    "$$\n",
    "\\frac{1}{N} \\sum\\limits_{\\alpha=1}^N \\vx^\\alpha = \\vec{0}\n",
    "$$\n",
    "\n",
    "**Approach**: Maximize the variance of the data after projection onto a vector $\\hat v$.\n",
    "\n",
    "* The estimated variance in the subspace along vector $\\hat v$ with  $\\|\\hat v\\| = 1$ is given by\n",
    "\n",
    "$$ F(\\hat v) = \\frac{1}{N} \\sum\\limits_{\\alpha=1}^N (\\vx^{\\alpha \\tau} \\hat v)^2 $$\n",
    "$$ = \\frac{1}{N} \\sum\\limits_{\\alpha=1}^N {\\hat v}^\\tau \\vx^\\alpha\\vx^{\\alpha \\tau} \\hat v$$\n",
    "$$ = {\\hat v}^\\tau \\underbrace{ \\left[ \\frac{1}{N} \\sum\\limits_{\\alpha=1}^N \\vx^\\alpha\\vx^{\\alpha \\tau}\\right] }_{\\text{estimated covariance matrix} C} \\hat v$$\n",
    "$$ = \\hat v^\\tau C \\hat v$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ],
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ee474ef4fc249feaff9c35777d8cdc1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='alpha', max=3.141592653589793, step=0.05), Output())â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# projection plot\n",
    "R = np.random.randn(200, 2)*[5,1]\n",
    "a = -np.pi/180*40\n",
    "X = (np.matrix([[np.cos(a), np.sin(a)], [-np.sin(a), np.cos(a)]])*R.transpose()).transpose()\n",
    "\n",
    "def pltprj(alpha=0.1):\n",
    "    plt.subplot(121)\n",
    "    plt.plot(X[:,0], X[:,1], \".\")\n",
    "    vec = [np.cos(alpha), np.sin(alpha)]\n",
    "    plt.plot([-10*vec[0], 10*vec[0]], [-10*vec[1], 10*vec[1]], 'r', lw=2)\n",
    "    plt.axis('equal')\n",
    "    plt.subplot(122)\n",
    "    prj = np.zeros(np.shape(X))\n",
    "    prj[:,0] = np.dot(np.matrix(X), vec)\n",
    "    prj[:,1] = np.dot(np.matrix(X), [-np.sin(alpha), np.cos(alpha)])\n",
    "    # plt.plot(prj[:,0], prj[:,1], \".\")\n",
    "    plt.plot(prj[:,0], 0*prj[:,0], \".\")\n",
    "    plt.axis([-15, 15, -10, 10]);\n",
    "    # plt.axis('equal')\n",
    "\n",
    "interact(pltprj, alpha=(0, np.pi, 0.05));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* $C$ is a positive semi-definite symmetric matrix. \n",
    "* Thus it exists a decomposition \n",
    "$$ C = UDU^\\tau$$ \n",
    "with \n",
    "* $U = [\\vec{u}_1,\\dots,\\vec{u}_d]$ being the matrix of eigenvectors of $C$ \n",
    "and \n",
    "* $D = \\text{diag}(\\lambda_1,\\dots,\\lambda_d):$ are the real-valued eigenvalues of $C$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Wanted**: \n",
    "$$\\sigma^2_{\\max} = \\max_{\\vec{w}} F(\\vec{w}) = \\max \\frac{\\vec{w}^\\tau C\\vec{w}}{\\vec{w}^\\tau \\vec{w}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Method**: \n",
    "Determine the zero crossings of all partial derivatives of $F(\\vec{w})$\n",
    "$$\n",
    "\\nabla_{\\vec{w}}F = \\frac{2 C \\vec{w}}{\\vec{w}^\\tau \\vec{w}} - \\frac{\\vec{w}^\\tau C \\vec{w}}{(\\vec{w}^\\tau \\vec{w})^2} 2\\vec{w} = 0\n",
    "$$\n",
    "* Note that here the product rule $(uv)' = u'v + uv'$ has been applied\n",
    "\n",
    "* We can reshape the equation as\n",
    "$$\n",
    "C\\vec{w} = \\underbrace{\\left[ \\frac{\\vec{w}^\\tau C \\vec{w}}{\\vec{w}^\\tau \\vec{w}}  \\right]} \\vec{w} ~~~~~ \\left| ~~~~~\\text{then multiply eq. with}~~ \\cdot 1/\\|\\vec{w}\\|\\right.$$\n",
    "$$ C\\hat w ~~~~~ = ~~~~~ \\lambda ~~~~~~ \\hat w ~~~~~~~~~~~~~~~~~~~~~~~~~~~~$$\n",
    "\n",
    "* Yet this is just the eigenvalue condition!\n",
    "* The necessary condition of stationarity leads to solutions that are the eigenvectors\n",
    "* For that reason $F$ is maximized by the eigenvector belonging to the largest eigenvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually we use a sorted order or eigenvalues: $\\lambda_1\\ge\\lambda_2\\ge\\dots\\lambda_d~(\\ge 0)$.\n",
    "\n",
    "Then we can simply write \n",
    "$$F_{\\max} = \\sigma^2_{\\max} = \\lambda_1$$\n",
    "\n",
    "With that, we can determine the first principal component of the data:\n",
    "\n",
    "$$ y_1^\\alpha = \\hat{u}_1^\\tau \\vx^\\alpha ~~~~~~~~~ \\text{(projection indices)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformation back into data space:\n",
    "$$ \\tilde{x}^\\alpha = \\hat{u}_1 y_1^\\alpha = \\underbrace{\\hat u_1 \\hat u_1^\\tau}_{\\text{Projektionsmatrix}}\\vx^\\alpha~\\text{(reconstruction)}$$\n",
    "\n",
    "Concerning the data compression \n",
    "* Coding using equation 'projection indices'\n",
    "* Decoding using equation 'reconstruction'\n",
    "\n",
    "The estimated variance along the first principal component is $\\sigma_1^2 = \\lambda_1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Total variance of the data distribution:**\n",
    "\n",
    "\\begin{eqnarray}\n",
    "V & = & \\sum\\limits_{j=1}^d \\text{Var}(x_j) = \n",
    "\\frac{1}{N} \\sum\\limits_{\\alpha=1}^N \\vx^{\\alpha \\tau}\\vx^\\alpha ~~~ \\text{(since mean 0)}\\\\\n",
    "& = & \\frac{1}{N} \\sum_\\alpha \\text{trace}(\\vx^\\alpha \\vx^{\\alpha\\tau})\\\\\n",
    "& = & \\text{trace}\\left[ \\frac{1}{N} \\sum_\\alpha \\vx^\\alpha \\vx^{\\alpha\\tau}\\right]\\\\\n",
    "& = & \\text{trace}(C)\\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But since $C=UDU^\\tau$ and $\\text{trace}(M) = \\text{trace}(U^\\tau MU) ~~ \\forall ~~ U$ orthonormal, it is \n",
    "$$\n",
    "V = \\text{trace}(D) = \\text{trace}(\\text{diag}(\\lambda_1,\\dots,\\lambda_d)) = \\sum_{i=1}^d \\lambda_i\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Iterative application to perpendicular subspace**\n",
    "\n",
    "Now we search the 1-dimensional subspace which contains the largest fraction of the remaining variance $\\sum_{i=2}^d \\lambda_i$. \n",
    "\n",
    "The decomposition yields for the orthogonal part \n",
    "$$\n",
    "\\vx = \\vx^{(1)} + (\\vx - \\vx^{(1)}) = \\hat u_1 \\hat u_1^\\tau \\vx + \\underbrace{( I - \\hat u_1 \\hat u_1^\\tau )}_{\\text{projection matrix}} \\vx\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we maximize \n",
    "\\begin{eqnarray}\n",
    "F^{(2)}(v) & = & \\frac{1}{N} \\sum_\\alpha^N \\hat v^\\tau\\left[(I-\\hat u_1\\hat u_1^\\tau)\\vx^\\alpha\\right]\\left[(I-\\hat u_1\\hat u_1^\\tau)\\vx^\\alpha\\right]^\\tau \\hat v \\\\\n",
    "& = & \\frac{1}{N} \\sum_\\alpha^N \\hat v^\\tau (I-\\hat u_1\\hat u_1^\\tau)\\vx^\\alpha\\vx^{\\alpha\\tau} (I-\\hat u_1\\hat u_1^\\tau) \\hat v\\\\\n",
    "& = & \\hat v^\\tau (I-\\hat u_1\\hat u_1^\\tau) C (I-\\hat u_1\\hat u_1^\\tau) \\hat v \\\\\n",
    "& = & \\hat v^\\tau (  C -  \\hat u_1\\hat u_1^\\tau C - C \\hat u_1\\hat u_1^\\tau + \\hat u_1\\hat u_1^\\tau C \\hat u_1\\hat u_1^\\tau) \\hat v\\\\\n",
    "& = & \\hat v^\\tau (  C -  \\hat u_1\\lambda_1\\hat u_1^\\tau  - \\underbrace{\\lambda_1 \\hat u_1\\hat u_1^\\tau + \\hat u_1\\lambda_1\\hat u_1^\\tau}_{=0}) \\hat v \\\\\n",
    "& = & \\hat v^\\tau (  C -  \\lambda_1\\hat u_1\\hat u_1^\\tau) \\hat v \\\\\n",
    "& = & \\hat v^\\tau \\left[ UDU^\\tau - U \\text{diag}(\\lambda_1,0,0\\dots,0) U^\\tau\\right] \\hat v\\\\\n",
    "& = & \\hat v^\\tau \\left[ U \\text{diag}(0, \\lambda_2,\\lambda_3\\dots,\\lambda_d) U^\\tau\\right] \\hat v\\\\\n",
    "& = & \\hat v^\\tau \\tilde{C} \\hat v\\\\\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see: \n",
    "* the solution is analog to finding the first principal component. The optimal vector now belongs to the largest eigenvalue of $\\tilde C$, and thus is $\\lambda_2$. \n",
    "\n",
    "* Thus $\\hat v_2 = \\hat u_2$ is the direction of the 2nd principal component with variance $\\sigma_2^2 = \\lambda_2$\n",
    "\n",
    "* In analogy, we obtain all further components as\n",
    "$$ \\hat v_j = \\hat u_j~,~ \\sigma_j^2 = \\lambda_j ~~ \\forall ~~ j = 1,\\ldots, d $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks**:\n",
    "* Principal components are uncorrelated!\n",
    "$$ \\text{corr}(y_j, y_k) = \\frac{1}{N} \\sum_\\alpha \\hat u_j^\\tau \\vx^\\alpha \\cdot  \\hat u_k^\\tau \\vx^\\alpha$$\n",
    "$$ = \\hat u_j^\\tau C \\hat u_k = \\lambda_k \\underbrace{\\hat u_j^\\tau \\hat u_k}_{=0~\\forall j \\not=  k}$$\n",
    "* The eigenvectors are pairwise orthogonal "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ws17EOT20171206]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary (PCA, general case of uncentered data sets)**:\n",
    "\n",
    "1. Compute the estimated covariance matrix $\\hat{C}$ of the data set $X = \\{\\vx^\\alpha\\}_{\\alpha = 1, \\dots, N}$\n",
    "$$ \\hat C =\\frac{1}{N-1} \\sum\\limits_{\\alpha=1}^{N} (\\vx^\\alpha - \\bar x)( \\vx^\\alpha-\\bar x)^\\tau \\quad(\\text{symmetric}~~ d \\times d \\mbox{-matrix}) $$\n",
    "2. Compute the eigenvalues $\\lambda_j$ and eigenvectors $\\hat u_j$ of $\\hat C$: \n",
    "$$ \\hat{C} \\hat u_j=\\lambda_j\\hat u_j $$\n",
    "Since $\\hat C$ is symmetric, $\\hat u_i^\\tau \\hat u_j= \\delta_{ij}$ can always be achieved.  \n",
    "Then it holds:\n",
    "\n",
    "a. Each data vector $\\vec{x}^\\alpha$ can be decomposed into its Eigenvector decomposition\n",
    "$$ \n",
    "\t\\vx^\\alpha = \\bar x + \\sum\\limits_{j=1}^{d} y_j^\\alpha \\hat u_j~, ~~~~~~~ \\text{( eigenvalue decomposition )} \n",
    "$$\n",
    "where the coefficients $y_j^\\alpha$ are given by \n",
    "$$\n",
    "\ty^\\alpha_j = \\hat u_j^\\tau (\\vx^\\alpha-\\bar x) ~~~~~~~\\text{( projection indices )}$$ \n",
    "\n",
    "b. $y_j^\\alpha$ are centered (i.e. mean 0) and pairwise uncorrelated and the eigenvalues  $\\lambda_i$ are the variances of the component:\n",
    "$$\t\\frac{1}{N-1} \\sum\\limits_\\alpha y_i^\\alpha y_j^\\alpha = \\lambda_i \\delta_{ij} $$\n",
    "\n",
    "c. The matrix $\\hat C$ can be represented by \n",
    "$$ \\hat C = U \\hat D U^\\tau $$\n",
    "* where $U=(\\hat u_1, \\ldots, \\hat u_d)$ has the eigen vectors as columns,\n",
    "* and $\\hat D = \\text{diag}(\\lambda_1,\\ldots, \\lambda_d)$ is a diagonal matrix of the eigenvalues \n",
    "\n",
    "**Proof**:\n",
    "\n",
    "a) follows from inserting the definitions\n",
    "\n",
    "b) \n",
    "\\begin{eqnarray}\n",
    "\\frac{1}{N-1}              \\sum\\limits_\\alpha y_i^\\alpha y_j^\\alpha \n",
    "& = & \\frac{1}{N-1}   \\sum\\limits_\\alpha (\\hat u_i \\cdot (\\vx^\\alpha-\\bar x)) \\cdot (\\hat u_j \\cdot (\\vx^\\alpha-\\bar x)) \\\\\n",
    "& = & \\frac{1}{N-1}  \\sum\\limits_\\alpha \\hat u_i^\\tau ((\\vx^\\alpha-\\bar x)  (\\vx^{\\alpha}-\\bar x)^\\tau \\hat u_j \\\\\n",
    "& = & \\hat u_i^\\tau \\hat C  \\hat u_j \\\\\n",
    "& = & \\hat u_i^\\tau \\lambda_j  \\hat u_j \\\\\n",
    "& = & \\delta_{ij}  \\lambda_j \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "c) is equivalent to $U^\\tau \\hat CU = \\text{diag}(\\lambda_1,\\ldots,\\lambda_d)$. \n",
    "* The $ij$ element of this equation is the last bit of the equation string of (b).\n",
    "\n",
    "**Interpretation**:\n",
    "* The eigenvector decomposition describes each data point (vector) by a new parameter vector $\\vec{y}^\\alpha=(y^\\alpha_1,\\ldots, y^\\alpha_d)$.\n",
    "\n",
    "* The $\\vec{y}^\\alpha$ are obtained by a linear transformation from the $\\vx^\\alpha$. However, the features are now pairwise uncorrelated. (yet not independent!!!)\n",
    "\n",
    "* The eigenvalues $\\lambda_j$ equal the variance of the respective component $y^\\alpha_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relevance for Dimensionality reduction**:\n",
    "\n",
    "W.l.o.g. let all eigenvectors be enumerated so that the eigenvalues form a descending series:\n",
    "$$ \\lambda_1 \\geq \\lambda_2 \\geq \\dots \\geq \\lambda_d \\geq 0 $$\n",
    "\n",
    "Then stopping the decomposition after the $q$-th term yields:\n",
    "$$ \\tilde{\\vx}^\\alpha = \\bar x + \\sum\\limits_{j=1}^q y_j^\\alpha \\hat u_j $$ \n",
    "with approximation error \n",
    "$$ \\vec{\\delta}_\\alpha=  \\sum\\limits_{j=q+1}^d y_j^\\alpha \\hat u_j. $$\n",
    "Note that this is the smallest possible approximation error when using only $q$ components!\n",
    "\n",
    "The vector $\\tilde{\\vx}^\\alpha$ can be regarded as orthogonal projection of $\\vec{x}^\\alpha$ on the $q$-dimensional subspace $\\text{span}\\{\\hat u_j | j=1,\\ldots, q\\}$. \n",
    "\n",
    "The total variance $\\hat \\sigma^2$ of $\\vec{\\delta}_\\alpha$ over all data is\n",
    "\\begin{eqnarray}\n",
    "\\hat \\sigma^2 & = &  \\frac{1}{N-1} \\sum\\limits_\\alpha \\vec{\\delta}_\\alpha^2\\\\\n",
    "& = & \\frac{1}{N-1} \\sum\\limits_{i>q}\\sum\\limits_{\\alpha} \\hat u_i^T \\hat u_i(y_i^\\alpha)^2 \\\\\n",
    "& = & \\frac{1}{N-1} \\sum\\limits_{i>q}\\sum\\limits_{\\alpha} 1\\cdot (y_i^\\alpha)^2 \\\\\n",
    "& = & \\sum\\limits_{i>q}\\left(\\frac{1}{N-1}\\sum\\limits_{\\alpha} (y_i^\\alpha)^2\\right) \\\\\n",
    "& = & \\sum\\limits_{i>q} \\lambda_i \\\\\n",
    "\\end{eqnarray}\n",
    "\n",
    "* That means that the expected approximation error is equal to the sum of the eigenvalues belonging to the unused eigenvectors.\n",
    "\n",
    "* Using the $q$ largest eigenvalues (i.e. projection on  $\\text{span}\\{\\hat{v}_j ~~ | ~~ j=1,\\ldots,q\\}$) thus minimizes the mean squared error (MSE) among all linear projections onto a $q$-dimensional linear subspace (called _Karhunen-Loeve-expansion_)\n",
    "\n",
    "* Choice of $q$: best according to the eigenvalue distribution of $\\hat{C}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfUAAAEKCAYAAAALjMzdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzt3Xl4VOX9/vH3hyTsIDs1iWwVUKpU\nlE2xSADZV8EICopLUX51wVYsWGnVahVw+aq1VFyKyBqRHSUKBKwIyFoRKUhBJUEFQQQxhCzP748Z\nMECAhMzJyUzu13XNNZmTM2dup71yc855znPMOYeIiIiEv1J+BxAREZHQUKmLiIhECJW6iIhIhFCp\ni4iIRAiVuoiISIRQqYuIiEQIlbqIiEiEUKmLiIhECJW6iIhIhIj2O0BB1ahRw9WrV8/vGCIiIkVi\n3bp13znnauZn3bAr9Xr16rF27Vq/Y4iIiBQJM/syv+vq8LuIiEiEUKmLiIhECJW6iIhIhFCpi4iI\nRAiVuoiISITwrNTN7HUz22Nmn57m92ZmL5jZdjP7xMwu9ypLXuZsSKPNU0upP3IhbZ5aypwNaUX5\n8SIichpjV4wlZWfKCctSdqYwdsVYnxKdXXHJ7OWe+kSgyxl+3xVoGHwMBcZ7mOUEczakMWrWJtIO\npOOAtAPpjJq1ScUuIhGnuJRNQbSIbUHizMTjuVN2ppA4M5EWsS18TnZ6xSWzZ9epO+c+MLN6Z1il\nNzDJOeeAVWZWxczOd8597VWmY8YlbyU9M5v9iycAUK3jUNIzsxmXvJU+zeK8/ngRkSJzrGyS+ieR\nUD/heNkk9U/y7DNzXA5Hs4+SkZVBRnbGOT33adyHHtN6cMX5V7Du63X0u7gfKV+kkPJFytkD+KTr\nhV3pMa0HreJasWnPpuPfeVHyc/KZOGBXrtepwWWnlLqZDSWwN0+dOnUK/cG7D6QDcHTPjjyXi3iq\nXbvA87JlfqaQCOac40jWEQ5mHCS+cjyPJzxO3xl96XJhF979/F3ubnU3Xxz4gn+u/edpi/Vo9tFz\nLuTMnMyQ/bf8+6t/AzD5k8kh26aXHI6UL1IY3XZ0kRc6+Fvqlscyl9eKzrkJwASA5s2b57lOQcRW\nKUdaHgUeW6VcYTctIhFs7IqxtIhtccIf65SdKazZvYYH2zxY6O3nuBx+PPojBzMOFuhx6OihU5Zl\n5WSdsv0Zm2cA8Ld//y3PzzeMMtFlKBNV5ozPVWOqnrr8LO8p6POqXasYPGcww5oPY/za8b7s9RbU\nsaMgxzIn1EsoUXvqqcAFuV7HA7uL4oNHdG7MqFmbTlhWLiaKEZ0bF8XHi0iYOt2h7CnXTWHfT/vy\nV8JH8yjljEPHyzk/yseUp3KZyic8GlRtQOUylalUutIpv/viwBc8/dHT9GvSj9lbZvNc5+doW7ft\nKWUaXSoas7z2t4pWys4UBs8ZfPx7TqiXcML3XhzlPq3hZ2Y/S30ecLeZTQdaAT8Uxfl04Ph581um\nRJGRlU1clXKM6NxY59NF5LiMrAx2H9pN6sHUEx4X17iYTpM7Ual0JQ4cOUB0qWg6T+581u0ZdkrZ\nVi1blbrn1T1hWV6lfMLvy1QiulT+/3Sn7Ezh/uT7mX3DbBLqJzDo0kHFviDX7F5zQr6E+gkk9U9i\nze41ynwWFhin5sGGzaYB7YAawLfAX4AYAOfcPy3wz8G/Exgh/xNwq3PurHdqad68uQvVDV3aBc9t\nLtO5TSlKOqfuu8NHD5N6MJW0Q2mnlPaxx96f9p7yvsplKhNXKY4jWUfYeWAnV5x/BR3qdzildPMq\n4woxFXzZC/b6lIF4z8zWOeea52ddL0e/DzzL7x3wO68+X0RKHuccP2T8ECjsgycV9qGffz5w5MAp\n761erjrxleOJqxxHi9gWxFeOP+ERVzmOymUqHz/MOrrtaMavHU+XC7sU271HIM/iTqhf9Od6pWiE\n3a1XRSQyFHQP0jnHdz99d9Y97MOZh094n2HUrlib+MrxXFjtQtrVbUdc5bgTC7tSHOVizj5Qtric\nNxU5HZV6GJmzIY1xyVvZfSCdWI0DCF8vv+x3gmIh96CztnXbMmvLLIYuGMr9re/nxdUvnrJ3nXYw\njYzsjBO2EWVRxFaKJb5yPE1rN6Vbw27HS/pYYZ9f6XxKR5UOSebict5U5HQ8O6fulZJ6Tv3YLHjp\nmdnHl5WLieLJ6y5VsUtYOZp9lI3fbGR16mrmbp1LyhcpOOdwJ13RWjqq9ImHwCudeCg8vnI8tSvU\nJqpUlE//JSJFo1icU5fQ0ix4EWT+/MBzz57+5igCzjm++uErVqetZlXqKlalrmL91+uP73HHVoql\nUfVG/Pe7/9Ltwm4MazHseHFXL1e9WFxeJRJOVOphQrPgRZBnngk8R2Cp/3j0R9buXsvq1NWsSguU\n+Dc/fgNA2eiyNI9tzj0t76F1fGtaxbfi832fnzDo7IGYB7jsF5f5/F8hEr5U6mFCs+BFkJkz/U4Q\nEjkuh63fbWVV6qrje+Kb9mwix+UA0LBaQ65tcG2gwONa0bR2U2KiYo6/X4POREJPpR4mNAteBKlR\nw+8E52TfT/uOl/fqtNWsTl3NDxk/AHBemfNoFd+K3o170zq+NS3jWlK9fPUzbk+DzkRCT6UeJjQL\nXgSZODHwPGSInynOKDM7k0++/SRwHjxtFatTV/P5/s8BKGWluLTWpQy4ZACt41vTOr41jao3opQV\n7E7Oun5aJPRU6mGkT7M4mtWpAsCyke19TiPnrBiWeurB1OMD2ValrmLd1+s4knUEgF9U/AWt41tz\ne7PbaR3fmitir6Bi6Yo+JxaRvKjURUqYnzJ/Yt3udcf3wlelrmL3ocC9lMpEleHy8y9nWPNhx/fC\nL6h8gUahi4QJlbpIBDjd7Gwfp31Mn4v6nHBJ2SfffkK2C8x30KBqA9rVa0fruECB//oXvw7ZRC0i\nUvRU6uI5zYTnvWOzs73e63VKR5Vm2qZpTP10KmWiyjByyUgAKpWuRMu4loy8euTxEek1K9T0ObmI\nhJJKXTx18kx4aQfSj4/iV7GHTkL9BP5w5R/oPb338ZnZ6lWpR8f6HY8fRr+oxkWafU0kwqnUxVOa\nCa9oTFg3gYeXPkytCrX49vC3PHjVg4y5dozfsUSkiBXsGhSRAso9E17u2fA0E15o5LgcHlryEHcu\nuJMrzr+CrJwsRrcdzesbXydlZ4rf8USkiKnUxVOnm/FOM+EVXkZWBoNmDeLJD5+kR6Me7Diwg7eu\nf4vHEh4jqX8SiTMTVewiJYxKXTw1onNjysWceB5XM+EV3v70/XSa3Ilpn07jqQ5PcfUFV592djYR\nKTl0Tl08pZnw8lDIud93fL+DblO6sfPATqZeN5WBlw7Mcz3NziZS8qjUxXOaCe8khZj7/eO0j+k5\nrSeZ2Zm8P/h92tZtG8JgIhLudPhdpKhNnPjzVLEFMPe/c2k3sR0VYirw0e0fqdBF5BQqdZGidg6l\n/uLqF+k7oy+X1r6Ulbev5KIaF3kSTUTCmw6/ixS1ZcvyvWqOy+GB9x7guVXP0btxb6b2m0r5mPLe\nZRORsKZSFymm0jPTGTR7ELO2zOLelvfybOdnNSOciJyRSl2kqD39dOD5gQdOu8rew3vpNb0Xq1NX\n81zn5xjeengRhRORcKZSFylqCxYEnk9T6tv2baPblG6kHUpjZuJMrrv4uiIMJyLhTKUuUoys+GoF\nvaf3xsxIuSWF1vGt/Y4kImFEo99Fiom3Nr9Fh0kdqFauGqtuX6VCF5ECU6mL+Mw5x7gV40icmUjz\n2OZ8dPtH/LLaL/2OJSJhSIffRXyUlZPFve/ey/i147m+yfVM6juJstFl/Y4lImFKpS7ik8NHDzPg\n7QEs2LaAEVeN4KmOT1HKdPBMRM6dp39BzKyLmW01s+1mNjKP39cxsxQz22Bmn5hZNy/ziBQX35Q+\nyjUTr+Gdz9/hH93+wdhrx6rQRaTQPNtTN7Mo4CXgWiAVWGNm85xzn+Va7WEgyTk33syaAO8A9bzK\nJJJfczakMS55K7sPpBMb4jvLfVb+MN2abuK776KZN2Ae3Rt1D8l2RUS8PPzeEtjunNsBYGbTgd5A\n7lJ3QOXgz+cBuz3MI5IvczakMWrWJtIzswFIO5DOqFmbAApd7Ck7U+h7+QbK5USxfMhyroi9otB5\nRUSO8fJ4XxywK9fr1OCy3B4BBplZKoG99Hs8zCOSL+OSt5Kemc3+xRPYv3gCAOmZ2YxL3lqo7U7+\nZDKdJ3cmLrYxqx7cpkIXkZDzstQtj2XupNcDgYnOuXigG/Cm2aknFs1sqJmtNbO1e/fu9SCqyM92\nH0gH4OieHRzds+OU5QXlnOOvy//K4NmDubrO1ay4bQV1q9QNSVYRkdy8LPVU4IJcr+M59fD67UAS\ngHNuJVAWqHHyhpxzE5xzzZ1zzWvWrOlRXJGA2CrlCrT8TDKzM7lj3h38edmfGdx0MIsGLaLK31/9\nef53EZEQ8rLU1wANzay+mZUGBgDzTlrnK6ADgJldTKDUtSsuvhrRuTHlYk68G1q5mChGdG5coO0c\nzDhIj2k9eH3j64xuO5o3+rxB6ajSsHJl4CEiEmKeDZRzzmWZ2d1AMhAFvO6c22xmjwFrnXPzgD8A\nr5jZ/QQOzQ9xzp18iF6kSB0bDHfLlCgysrKJO4fR76kHU+k+tTuf7f2M13q9xm3Nbvv5l2+/HerI\nIiKAx5PPOOfeITAALveyP+f6+TOgjZcZRM5Fn2ZxNKtTBYBlI9sX6L3/+eY/dJ/anYMZB1l440I6\n/bKTFxFFRE6h2S5EQih5ezK/+ddvAPjwtg/zLvRRowIPEZEQ0zSxIiHy2vrXuHPBnVxS6xIW3riQ\nuMqnOVyv8+ki4hHtqYsUknOOh5c+zB3z76Bjg458cOsHpy90EREPaU9dpBAysjK4fd7tTNk0hTua\n3cE/uv+DmKgYv2OJSAmlUhc5R9+nf891Sdex7ItlPJ7wOA/95iHM8ppzSUSkaKjURc7BFwe+oNuU\nbmzfv53JfSdzU9Ob/I4kIqJSFymotbvX0mNqDzKyM3hv8Hu0q9fO70giIoAGyokUyIJtC7hm4jWU\niynHR7d9pEIXkWJFpS6ST/9Y8w96T+/NxTUuZuXtK7m45sV+RxIROYEOv4ucRY7L4Y/v/5GnVz5N\nz0Y9mdZvGhVKVzj3DVavHrpwIiK5qNRFTjJ2xVhaxLYAILtUNjfMvIGZn83kyvgrmX3DbKJKRZ1l\nC2ehud9FxCMqdZGTtIhtQeLMRGrXqM2uOrv48LMPqRBTgccTHi98oYuIeEjn1EVOklA/gRe7vsjm\nSzZzsNJBKpWuxPyB82nfoGA3djktzf0uIh7RnrrISXZ+v5ORi0diOYaLcgxvPZyE+gmh+4B9+0K3\nLRGRXFTqIrls37+d9m+05/v074nKiSJ2Vyzjy48noV5C6Ip9woTQbEdE5CQ6/C4StG3fNq6ZeA0/\nHPmB6KhomnzahPo765PUP4nEmYmk7EzxO6KIyBmp1EWALXu3cM3Ea8jMzuS2ZrcxK3EWVQ9UBQLn\n2JP6J7Fm95rQfNjQoYGHiEiI6fC7lHif7vmUDpM6YBjLhiyjSc0mADzKo8fXSagfwsPv27aFZjsi\nIifRnrqUaP/55j8kvJFAdKlolg9ZfrzQRUTCkUpdSqz1X6+n/aT2lI0uy/Ihy2lco7HfkURECkWl\nLiXSmrQ1dJjUgYqlK7J8yHIurHah35FERApNpS4lzspdK+n4Zkeqlq3KB0M+oEHVBn5HEhEJCZW6\nlCgffvUhnSZ3olaFWiwfspy6Ver6HUlEJGRU6lJiLP9iOV0mdyGuUhzLhyzngvMu8DuSiEhIqdSl\nRFiyYwldp3SlbpW6LBuyjNhKsX5HEhEJOV2nLhEveXsyfWb0oWG1hiy+eTG1KtTyN1CjRv5+vohE\nLJW6RLSF2xZyXdJ1NKnZhPcHv0+N8jX8jqS530XEMzr8LhFr7n/n0ndGXy6tdSlLbl5SPApdRMRD\nKnWJSG9/9jb93+pPs/ObsfjmxVQrV83vSD/T3O8i4hEdfpeIM+PTGdw06yZaxbfinRvf4byy5/kd\n6UTVq/udQEQilEpdIsqUT6Zw85ybaXNBGxbeuJBKZSr5HelUTz7pdwIRiVCeHn43sy5mttXMtpvZ\nyNOsk2hmn5nZZjOb6mUeiWxvbHyDwbMHc03da3j3pneLZ6GLiHjIsz11M4sCXgKuBVKBNWY2zzn3\nWa51GgKjgDbOue/NzOdrjSRcvbr+VYbOH0rHBh2ZM2AO5WPK+x3p9Pr1Czy//ba/OUQk4ni5p94S\n2O6c2+GcOwpMB3qftM5vgZecc98DOOf2eJhHItT4NeP57fzf0vnCzswbOK94FzrAvn2Bh4hIiHlZ\n6nHArlyvU4PLcmsENDKzFWa2ysy65LUhMxtqZmvNbO3evXs9iivh6MXVL/L/3vl/9GjUgzk3zKFs\ndFm/I4mI+OaspW4Bg8zsz8HXdcysZT62bXkscye9jgYaAu2AgcCrZlbllDc5N8E519w517xmzZr5\n+GgpCZ5d+Sz3LrqXvhf15e3EtykTXcbvSCIivsrPnvo/gCsJlC7AIQLnys8mFch9x4x4YHce68x1\nzmU653YCWwmUvMgZjflwDH947w9c3+R6ZvSfQemo0n5HEhHxXX5KvZVz7nfAEYDg+e/8/AVdAzQ0\ns/pmVhoYAMw7aZ05QAKAmdUgcDh+Rz6zSwn1+AePM3LJSAZeMpCp/aYSExXjdyQRkWIhP6WeGRzJ\n7gDMrCaQc7Y3OeeygLuBZGALkOSc22xmj5lZr+BqycA+M/sMSAFGOOc0gkjy5JzjLyl/YXTKaAY3\nHcybfd8kupSmWhAROSY/fxFfAGYDtczsCaA/8HB+Nu6cewd456Rlf871swN+H3yInJZzjj8t/RNP\nfvgkt152K6/0fIWoUlF+xxIRKVbOWurOuSlmtg7oQGDwWx/n3BbPk4kEOed48P0HeXrl0wy9fCjj\ne4ynlOm2BSIiJztrqZtZa2Czc+6l4OtKZtbKObfa83RS4jnnuD/5fp5f/Ty/a/E7Xuj6QvgX+pVX\n+p1ARCJUfg6/jwcuz/X6cB7LREIux+Vw77v38tKalxjeajjPdn4Ws7yulAwzmvtdRDySn1K34Llv\nAJxzOWam0UniqRyXw7AFw5iwfgIjrhrBmI5jIqPQRUQ8lJ/jmDvM7F4ziwk+7kOXnYmHsnOyuWPe\nHUxYP4GHrn4o8gq9X7+f538XEQmh/JT6XcBVQBqByWJaAUO9DCUlV3ZONrfOvZV/bfwXf7nmLzze\n/vHIKnQInFPXeXUR8UB+Rr/vITBxjIinsnKyGDx7MNM/nc5fE/7Kw23zdeVk+HngAb8TiEiEys/o\n95oE7qZWL/f6zrnbvIslJU1mdiY3zrqRmZ/N5KkOT/HHq//odyQRkbCTnwFvc4F/A4uBbG/jSEl0\nNPsoA2YOYPZ/Z/Nsp2e5/8r7/Y7krXbtAs/LlvmZQkQiUH5KvbxzTrtN4omMrAyuf+t65m+bzwtd\nXuCeVvf4HUlEJGzlZ6DcAjPr5nkSKXHSM9PpM6MP87fNZ3z38Sp0EZFCyk+p30eg2NPN7KCZHTKz\ng14Hk8j2U+ZP9Jrei+TtybzS8xXuan6X35FERMJefka/VyqKIFJyHD56mJ7TerLsi2X8q/e/uOWy\nW/yOJCISEfI1M5yZVQUaAmWPLXPOfeBVKIlchzIO0X1qd1bsWsGbfd/kpqY3+R1JRCRi5OeStjsI\nHIKPBzYCrYGVQHtvo0kkGLtiLC1iWwCQFZVFlyldWLVrFQMuHaBCFxEJsfzsqd8HtABWOecSzOwi\n4FFvY0mkaBHbgsSZidSuXpuv6n3F4dTDVCxdkTua3eF3NBGRiJOfgXJHnHNHAMysjHPuv0Bjb2NJ\npEion8BrPV9j8yWbOVTpEBVLV2TOgDkk1E/wO5qISMTJz556qplVAeYA75vZ98Bub2NJpDiYcZAn\nPnwCDDC4r9V9KvQePfxOICIRKj+j3/sGf3zEzFKA84BFnqaSiHD46GG6T+3O2t1ricqOIi41jvHl\nx5NQL6FkF7vmfhcRj5z28LuZVQ4+Vzv2ADYBHwIViyifhKn0zHR6Te/Fiq9WUCGmAr/a9Cvq76xP\nUv8kEmcmkrIzxe+IIiIR50zn1KcGn9cBa/N4FslTRlYG1yVdR8rOFBJ/lcjcAXOpeqAqEDjHntQ/\niTW71/ic0kft2v08/7uISAid9vC7c66HBW5kfY1z7qsizCRhLDM7kxtm3sCi7Yt4pecr3HF5YJT7\no7kumEioX8IPvw8Z4ncCEYlQZzyn7pxzZjYbuKKI8kgYy8rJYtDsQczdOpcXu754vNDlJCp1EfFI\nfi5pW2VmLTxPImEtx+Vw29zbSNqcxLhrx3F3y7v9jlR8ffdd4CEiEmL5uaQtAbjTzL4EDhO4OMk5\n55p6mkzChnOOuxbcxZufvMlfE/7KA1dpdPcZ9e8feNb91EUkxPJT6l09TyFhyznHfYvu45X1r/DQ\n1Q/xcNuH/Y4kIlJi5ec69S8BzKwWuW7oIuKc44+L/8iLH7/I/a3v5/H2j/sdSUSkRDvrOXUz62Vm\nnwM7geXAF8C7HueSMPDIskcY99E4hjUfxjOdniFwsYSIiPglPwPl/krgzmzbnHP1gQ7ACk9TSbH3\n1IdP8dgHj3HbZbfx925/V6GLiBQD+Sn1TOfcPqCUmZVyzqUAl3mcS4qx/1v1f4xaMoobL72RCT0n\nUMry838jERHxWn7+Gh8ws4rAv4EpZvY8kJWfjZtZFzPbambbzWzkGdbrb2bOzJrnL7b45Z9r/8n9\nyffT7+J+vNHnDaJKRfkdSUREgvJT6h8AVQjcV30R8D+g59neZGZRwEsERs83AQaaWZM81qsE3Aus\nzn9s8cO/NvyLYQuH0aNRD6b2m0p0qfxcPCEiIkUlP6VuQDKwjMCNXGYED8efTUtgu3Nuh3PuKDAd\n6J3Hen8FxgJH8pVYfDFt0zRun3c71za4lreuf4vSUaX9jiQiIic5a6k75x51zv0K+B0QCyw3s8X5\n2HYcsCvX69TgsuPMrBlwgXNuQf4jS1GbtWUWg2cPpm3dtswZMIey0bqysVCGDNFUsSLiiYIcP90D\nfAPsA2rlY/28hkO74780KwU8Bww564bMhgJDAerUqZOPj5ZQWbhtIQNmDqBlXEvmD5xP+ZjyfkcK\nfyp0EfFIfq5TH2Zmy4AlQA3gt/mcIjYVuCDX63hgd67XlYBLgGVm9gWBy+bm5TVYzjk3wTnX3DnX\nvGbNmvn4aAmFxTsW0y+pH01rN+Xdm96lUplKfkeKDJr7XUQ8kp899brAcOfcxgJuew3Q0MzqA2nA\nAODGY790zv1A4B8JAAT/4fCAc073ai8GPvjyA3pN60Wj6o1IHpTMeWXP8ztS5NDc7yLikfxME3va\nS9HO8r4sM7ubwCC7KOB159xmM3sMWOucm3cu2xXvrdy1ku5Tu1O3Sl0W37yY6uWr+x0psvzhD34n\nEJEI5ek1Sc65d4B3Tlr259Os287LLJI/63avo+uUrtSuUJslNy+hVoX8DJ+QAul51itCRUTOiaYC\nk+M++fYTOk3uRJWyVVh6y1JiK8X6HSkybd0aeIiIhJhmDxEAtuzdQsdJHSkXXY6ltyylznm6ysAz\nd94ZeNY5dREJMe2pC9v3b6fDpA6UslIsuXkJDao28DuSiIicA+2pl3BfHviSDpM6cDT7KMuGLKNx\njcZ+RxIRkXOkUi/B0g6m0X5Sew5mHGTpzUu5pNYlfkcSEZFCUKmXUN/++C0dJnVg7+G9vD/4fZqd\n38zvSCIiUkgq9RLou5++o+ObHdl1cBfJg5JpFd/K70giIhICKvUS5vv07+n0Zic+3/c5C29cyNV1\nrvY7koiIhIhKvQQ5mHGQrlO68umeT5k7YC4dGnTwO5KIiISQSr2EOHz0MN2ndmft7rXMTJxJ14Zd\n/Y4kIiIhplIvAdIz0+k9vTcf7fqIqddNpc9FffyOVLJp7ncR8YhKPcJlZGXQ/63+LN25lIl9JnLD\nJTf4HUk097uIeESlHsEyszMZ8PYA3vn8HV7u8TI3//pmvyMJ/Dzve2NN9CMioaVSj1DZOdkMnj2Y\nOf+dw/NdnmfoFUP9jiTHaO53EfGISj0C5bgcbpt3GzM2z2BMxzHc2+pevyNJbn/7m98JRCRCqdQj\njHOOYQuGMek/k3i03aM82OZBvyPJya66yu8EIhKhdJe2COKcY/ii4UxYP4GRbUYyuu1ovyNJXj76\nKPAQEQkx7alHCOcco5aM4oWPX+C+Vvfxtw5/w8z8jiV5eeihwLPOqYtIiGlPPUI8tvwxxqwYw11X\n3MVznZ9ToYuIlEAq9Qgw5sMxPLL8EYZcNoSXur+kQhcRKaFU6mHu+VXPM3LJSAZcMoBXe75KKdP/\npCIiJZUaIIy9vPZlhicPp+9FfZnUZxJRpaL8jiQiIj7SQLkwMXbFWFrEtjj++o2Nb3DXwrtoXL0x\n0/tPJyYqxsd0IiJSHKjUw0SL2BYkzkwktkosmaUzuXXurcSUiuH5Ls9TOqq03/FERKQYUKmHiYT6\nCST1T+Lag9eSHZVNdKlo5gyYQ+cLO/sdTUREigmdUw8jUaWiyC6VDQa/b/17ujXs5nckEREpRrSn\nHiY2frORrlO6gkH8V/G8Xv51ulzYhYT6CX5Hk4LS3O8i4hHtqYeB/+3/H+3faM+RrCNctPkifvm/\nX5LUP4nEmYmk7EzxO54U1FVXaf53EfGESr2Y+/rQ11z75rUcyTrCa71eo/be2sDP59jX7F7jc0Ip\nMM39LiIe0eH3YuzAkQN0mdKFPYf3kHJLCq3iWzGRicd/n1A/QYffw5HmfhcRj6jUi6mfMn+i57Se\nbNm7hQU3LqBVfCu/I0movPyy3wlEJEJ5Wupm1gV4HogCXnXOPXXS738P3AFkAXuB25xzX3qZKRxk\nZmdyw8wbWPHVCqb3n06nX3byO5KEUuPGnmx2zoY0xiVvZfeBdGKrlGNE58b0aRbnyWeJSPHk2Tl1\nM4sCXgK6Ak2AgWbW5KTVNgDNnXNNgZnAWK/yhIscl8Md8+9gwbYFvNTtJRJ/leh3JAm1+fMDjxCa\nsyGNUbM2kXYgHQekHUhn1KzgBFBFAAAOr0lEQVRNzNmQFtLPEZHizcuBci2B7c65Hc65o8B0oHfu\nFZxzKc65n4IvVwHxHuYp9pxzjHhvBJP+M4lH2z3KsBbD/I4kXnjmmcAjhMYlbyU9M5v9iyewf/EE\nANIzsxmXvDWknyMixZuXh9/jgF25XqcCZzoxfDvwbl6/MLOhwFCAOnXqhCpfsTNmxRieXfUsd7e4\nm9FtR/sdR8LI7gPpABzdsyPP5SJSMni5p57XTb1dniuaDQKaA+Py+r1zboJzrrlzrnnNmjVDGLH4\neHX9q4xaMoqBlwzk+a7P657oUiCxVcoVaLmIRCYvSz0VuCDX63hg98krmVlH4E9AL+dchod5iq1Z\nW2Zx54I76fzLzkzsM1H3RJcCG9G5MeViTrz1brmYKEZ09mZQnogUT14efl8DNDSz+kAaMAC4MfcK\nZtYMeBno4pzb42GWYitlZwoD3x5Iy7iWvJ34tu64Jufk2Cj3W6ZEkZGVTZxGv4uUSJ6VunMuy8zu\nBpIJXNL2unNus5k9Bqx1zs0jcLi9IvBW8HDzV865Xl5lKm7Wf72e3tN7c2G1C1l440IqlK7gdyQJ\nY32axdGsThUAlo1s73MaEfGDp9epO+feAd45admfc/3c0cvPL8627dtGl8ldqFquKsmDkqlWrprf\nkUREJMzp5K0P0g6m0enNTjgc7w9+n/jKJfpKPhERCRFNE1vE9qfvp/PkzuxL38eyW5bRqHojvyOJ\niEiEUKkXocNHD9Njag8+3/857970LlfEXuF3JPGD5n4XEY+o1ItIZnYm1791PavTVpPUP4n29TWQ\nqcTyaO53ERGVehHIcTkMmTuEd7e/y4QeE+jXpJ/fkcRPx+Z979nT3xwiEnFU6h5zznH/ovuZumkq\nT7R/gt9e8Vu/I4nfjs37rlIXkRBTqXvsiX8/wQsfv8DwVsMZdfUov+NIcTBzpt8JRCRCqdQ9NH7N\neEanjGZQ00E80/kZzecuATVq+J1ARCKUrlP3SNLmJH73zu/o3rA7r/d6XfO5y88mTgw8RERCTE3j\ngff/9z6DZg2iTZ02JF2fRExUjN+RpDhRqYuIR1TqIfZx2sf0ndGXi2pcxPyB8ykfU97vSCIiUkLo\nnHoIbdm7hW5TulGrQi2SByVTpWwVvyOJFGtzNqQxLnkruw+kE6s7y4kUmko9RHb9sItOkzsRXSqa\n9wa/x/mVzvc7kkixNmdDGqNmbSI9MxuAtAPpjJq1CUDFLnKOdPg9BL776Ts6Te7EwYyDLBq0iAur\nXeh3JJFib1zyVtIzs9m/eAL7F08AID0zm3HJW31OJhK+tKdeSD8e/ZHuU7uz8/udJA9K5rJfXOZ3\nJJGwsPtAOgBH9+zIc7mIFJz21AshIyuD62Zcx9rda5nRfwbX1LvG70giYSO2SrkCLReRs1Opn6Ps\nnGxunnMz7+94n1d7vkrvi3r7HUkkrIzo3JhyMVEnLCsXE8WIzrrhjci50uH3c+Cc45537yFpcxJj\nO47l1ma3+h1JJOwcGwx3y5QoMrKyidPod5FCU6mfg0eWPcL4teMZcdUIRrQZ4XccCTea+/24Ps3i\naFYncOnnspG6HbFIYanUC+jF1S/y2AePcetltzKm4xi/40g40tzvIuIRnVMvgGmbpnHvonvp3bg3\nE3pO0A1a5NxomlgR8Yj21PNp0fZF3DznZtrWbcu0ftOILqWvTs7RsUIfMsTPFHKONAueFGdqpnxY\nuWsl/ZL6cUmtS5g3YB7lYnTJjRTCsmV+J5BzpFnwpLjT4fez2LxnM92nduf8iuez6KZFnFf2PL8j\niYhPNAueFHfaUz+DLw98SefJnSkTXYb3Br9H7Yq1/Y4kkeDppwPPDzzgbw4pMM2CJ8Wd9tRPY+/h\nvXSa3InDmYdJHpRMg6oN/I4kkWLBgsBDwo5mwZPiTqWeh0MZh+g6pSu7ftjFgoELaFq7qd+RRKQY\nCNdZ8OZsSKPNU0upP3IhbZ5aypwNaX5HEo/o8PtJjmQdoc+MPmz8ZiNzB8ylTZ02fkcSkWIiHGfB\n0+C+kkWlnkt2TjY3zbqJpTuX8mbfN+neqLvfkUSkmAm3WfByD+4DqNZx6PHBfSr1yKNSD3LOMWzh\nMGZtmcVznZ9jUNNBfkcSESk0De4rWTw9p25mXcxsq5ltN7ORefy+jJnNCP5+tZnV8zLPmfxp6Z94\nZf0rPHT1QwxvPdyvGCIiIaXBfSWLZ3vqZhYFvARcC6QCa8xsnnPus1yr3Q5875y70MwGAGOAG7zK\ndMzYFWNpEdvi+OvnVj7Hkx8+ScvYljze/nGvP15EpMiM6Nz4+Dn0Y8JlcF+4zdxXHDJ7efi9JbDd\nObcDwMymA72B3KXeG3gk+PNM4O9mZs4552EuWsS2IHFmIrFVYskok8Hv3/s9paNK80SHJzSfu4hE\nFA3uKxrFJbN51Z9m1h/o4py7I/h6MNDKOXd3rnU+Da6TGnz9v+A6351uu82bN3dr164tdL6UnSl0\ne60jR6JziMZ4bXd7rkmPLfR2vbZx40YALrvsMp+TFEw45vYqc+333gPg206dQrpd0PdcVJTZWxu+\nOkBGVvYpy8tERx0fpFjc5M6847w4Xm7aG4C4KuVYUcgBlWa2zjnXPD/rermnntcu78n/gsjPOpjZ\nUGAoQJ06dQqfDEion8BV+85n6S/SuGN/k7AodICKFSv6HeGchGNurzJ7UebH6HsuGsrsrbwK/UzL\ni4PTZSvqAYlelnoqcEGu1/HA7tOsk2pm0cB5wP6TN+ScmwBMgMCeeijCpexM4ZMGGYxuPprxa8eT\nOPxWEuonhGLTnqrrd4BzFI65lbloKHPRCKfMNz61lLQ8yjCuSjluKKaXEZ4uc1EPSPRy9PsaoKGZ\n1Tez0sAAYN5J68wDbgn+3B9Y6vX5dAgUeuLMRJL6J/FYwmMk9U8icWYiKTtTvP5oERE5i3Ccua+4\nZPas1J1zWcDdQDKwBUhyzm02s8fMrFdwtdeA6ma2Hfg9cMplb15Ys3sNSf2Tju+ZJ9RPIKl/Emt2\nrymKjxcRkTPo0yyOJ6+7lLgq5TACe+hPXndpsR0kB8Uns2cD5bwSqoFyIiIi4aAgA+V0QxcREZEI\noVIXERGJECp1ERGRCKFSFxERiRAqdRERkQgRdqPfzWwv8KXfOXxWAzjtVLoSUvqui4a+56Kh77lo\nhPp7ruucq5mfFcOu1AXMbG1+L2+QwtF3XTT0PRcNfc9Fw8/vWYffRUREIoRKXUREJEKo1MPTBL8D\nlCD6rouGvueioe+5aPj2PeucuoiISITQnrqIiEiEUKmHETO7wMxSzGyLmW02s/v8zhTJzCzKzDaY\n2QK/s0QqM6tiZjPN7L/B/19f6XemSGVm9wf/bnxqZtPMrKzfmSKBmb1uZnvM7NNcy6qZ2ftm9nnw\nuWpR5VGph5cs4A/OuYuB1sDvzKyJz5ki2X0Ebhss3nkeWOScuwj4Nfq+PWFmccC9QHPn3CVAFDDA\n31QRYyLQ5aRlI4ElzrmGwBKK6LbioFIPK865r51z64M/HyLwB7D43mA4jJlZPNAdeNXvLJHKzCoD\nbYHXAJxzR51zB/xNFdGigXJmFg2UB3b7nCciOOc+APaftLg38Ebw5zeAPkWVR6UepsysHtAMWO1v\nkoj1f8CDQI7fQSJYA2Av8K/gaY5XzayC36EikXMuDXga+Ar4GvjBOfeev6kiWm3n3NcQ2BkDahXV\nB6vUw5CZVQTeBoY75w76nSfSmFkPYI9zbp3fWSJcNHA5MN451ww4TBEepixJgud0ewP1gViggpkN\n8jeVeEGlHmbMLIZAoU9xzs3yO0+EagP0MrMvgOlAezOb7G+kiJQKpDrnjh1tmkmg5CX0OgI7nXN7\nnXOZwCzgKp8zRbJvzex8gODznqL6YJV6GDEzI3D+cYtz7lm/80Qq59wo51y8c64egcFES51z2qsJ\nMefcN8AuM2scXNQB+MzHSJHsK6C1mZUP/h3pgAYlemkecEvw51uAuUX1wdFF9UESEm2AwcAmM9sY\nXPaQc+4dHzOJFMY9wBQzKw3sAG71OU9Ecs6tNrOZwHoCV9FsQLPLhYSZTQPaATXMLBX4C/AUkGRm\ntxP4B9X1RZZHM8qJiIhEBh1+FxERiRAqdRERkQihUhcREYkQKnUREZEIoVIXERGJECp1ESk2zGy4\nmZX3O4dIuNIlbSJSbARn8WvunPvO7ywi4Uh76iJhzsxuNrNPzOw/ZvammdU1syXBZUvMrE5wvYlm\nNt7MUsxsh5ldE7wX9BYzm5hrez+a2TNmtj74/prB5ZeZ2argdmcfu0e0mS0zszFm9rGZbTOz3wSX\nR5nZODNbE3zPncHl7YLvOXYf9SkWcC+BeclTzCyliL9GkYigUhcJY2b2K+BPQHvn3K8J3AP+78Ak\n51xTYArwQq63VAXaA/cD84HngF8Bl5rZZcF1KgDrnXOXA8sJzJAFMAn4Y3C7m3ItB4h2zrUEhuda\nfjuBu4G1AFoAvzWz+sHfNQuu24TA3draOOdeIHA70ATnXELhvhmRkkmlLhLe2gMzjx2uds7tB64E\npgZ//yZwda7157vAObdNwLfOuU3OuRxgM1AvuE4OMCP482TgajM7D6jinFseXP4GgXuhH3Ps5kLr\ncm2nE3BzcErj1UB1oGHwdx8751KDn70x13tEpBA097tIeDPgbANjcv8+I/ick+vnY69P9/cgPwNv\njm0rO9d2DLjHOZece0Uza3fSZ+d+j4gUgvbURcLbEiDRzKoDmFk14CMCd5cDuAn4sIDbLAX0D/58\nI/Chc+4H4Ptj58sJ3FhoeV5vziUZGBa8XTBm1sjMKpzlPYeASgXMKyJB+texSBhzzm02syeA5WaW\nTeDuW/cCr5vZCGAvBb/z2WHgV2a2DvgBuCG4/Bbgn8FLzvJzR7VXCRxWXx+83edeoM9Z3jMBeNfM\nvtZ5dZGC0yVtInICM/vROVfR7xwiUnA6/C4iIhIhtKcuIiISIbSnLiIiEiFU6iIiIhFCpS4iIhIh\nVOoiIiIRQqUuIiISIVTqIiIiEeL/A905t/FsVUf8AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 576x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lambdas = [1, 0.95, 0.9, 0.8, 0.2, 0.1, 0.05, 0.025, 0.0, 0.0]\n",
    "cdf = [np.sum(lambdas[:d]) for d in np.arange(len(lambdas))]\n",
    "ii = np.arange(len(lambdas))+1\n",
    "plt.stem(ii, lambdas, \"k-\")\n",
    "plt.plot([4.5, 4.5], [0,1], \"r-.\")\n",
    "plt.plot(ii, cdf/max(cdf), \"gx-\")\n",
    "plt.xlabel(\"component\"); plt.ylabel(\"variance\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Eigenvalue analysis provides important information about the intrinsic data dimensionality\n",
    "* intrinsic dimensionality can of course be much lower!\n",
    "\n",
    "**Remarks**:\n",
    "* Eigenvalue analysis is purely variance-driven \n",
    "* no statement is made about the semantic content in the dimension\n",
    "* non-linear structures are 'by principle' not findable using PCA\n",
    "* Few large eigenvalues can maybe only contain useless noise...\n",
    "* Practical procedure requires particular tricks if very-high-dimensional data are given \n",
    " * e.g. images where $d$ may be up to $10^6$."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "widgets": {
   "state": {
    "1bba81e76cfb4f95bf0d8393e0343a0c": {
     "views": [
      {
       "cell_index": 3
      }
     ]
    }
   },
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
