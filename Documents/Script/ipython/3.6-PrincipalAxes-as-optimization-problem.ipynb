{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## settings \n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy, scipy.stats\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\usepackage{amssymb} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\vx}{\\vec{x}} \\newcommand{\\vw}{\\vec{w}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6. Generalization: Principal Axis as Optimization problem\n",
    "\n",
    "Good descriptions often result from a minimization of an error function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.1 The Mean as Principal Point\n",
    "\n",
    "The mean vector \n",
    "    $$\\vec{\\mu} = \\langle \\vx \\rangle_{\\vx} = \\int\\limits_{\\R^d} \\vx p(\\vx) d\\vx$$ \n",
    "    ($ = \\frac{1}{N}\\sum_\\alpha \\vec{x}^\\alpha$ in the case of a discrete dataset)\n",
    "    \n",
    "minimizes the squared error $E(\\vw)$:\n",
    "$$ \\vec{\\mu} = \\arg \\min_{\\vw} \\underbrace{\\langle \\|  \\vx - \\vw \\|^2 \\rangle_{p(x)}}_{E(\\vw)} $$\n",
    "\n",
    "Proof:\n",
    "$$  E(\\vw) = \\langle(\\vx-\\vw)^\\tau (\\vx-\\vw)\\rangle_{\\vx} = \n",
    "\\langle\\vx^\\tau\\vx\\rangle - 2 \\vw^\\tau \\langle\\vx\\rangle + \\vw^\\tau\\vw $$\n",
    "\n",
    "A minimum has to fulfill the conditions for stationarity $\\nabla_{\\vw} E = \\vec{0}$, i.e. here\n",
    "\n",
    "$$\\nabla_{\\vw} E = -2\\langle\\vx\\rangle + 2\\vw = \\vec{0} \\Leftrightarrow \\vw = \\langle\\vx\\rangle ~~~ \\text{q.e.d.} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e788f2641704f15959f241c11c90d7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=1.0, description='wx', max=4.0, min=-4.0), FloatSlider(value=1.0, descâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# principal point example: \n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "d = np.random.randn(100, 2)\n",
    "\n",
    "def pltprj(wx=1, wy=1):\n",
    "    plt.figure(figsize=(4,4))\n",
    "    plt.plot(d[:,0], d[:,1], \"o\", markersize=5)\n",
    "    plt.axis([-4,4,-4,4])\n",
    "    E = 0 \n",
    "    for r in d: \n",
    "        E += (r[0]-wx)**2 + (r[1]-wy)**2\n",
    "        plt.plot([r[0],wx], [r[1],wy], 'r-', linewidth=0.5)\n",
    "    print(\"Error: \", E)\n",
    "\n",
    "interact(pltprj, wx=(-4, 4, 0.1), wy=(-4, 4, 0.1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.2. Principal Axes\n",
    "\n",
    "**Given**:\n",
    "* centered data (with mean vector $\\langle \\vx \\rangle = \\vec{0}$)\n",
    "\n",
    "**Wanted**:\n",
    "* Axis that minimizes the squared distance to the data\n",
    "$$\n",
    "E(\\vw) = \\frac{1}{N} \\sum\\limits_{\\alpha=1}^N\\min_y\\left[\n",
    "  \\| \\vx^\\alpha - \\vw\\cdot y\\|^2\n",
    "\\right] = \\frac{1}{N} \\sum\\limits_{\\alpha=1}^N \\min_y F_\\alpha(y)\n",
    "$$\n",
    "<img src=\"images/PCA-as-min-problem.png\" width=\"50%\">\n",
    "\n",
    "with $$F_\\alpha(y) =  \\| \\vx^\\alpha - \\vw\\cdot y\\|^2 = (\\vx^\\alpha - \\vw\\cdot y)^2$$\n",
    "\n",
    "<img src=\"images/PCA-as-min-problem-2.png\" width=\"20%\">\n",
    "\n",
    "\n",
    "** Step S1**: Minimize\n",
    "\\begin{eqnarray}\n",
    "F_\\alpha(y) &=& (\\vx^\\alpha - \\vw y)^\\tau (\\vx^\\alpha - \\vw y)\\\\\n",
    "            &=& \\vx^{\\alpha \\tau}\\vx^\\alpha  - 2 \\vx^{\\alpha\\tau} \\vw y + \\vw^\\tau\\vw y^2\n",
    "\\end{eqnarray}\n",
    "\n",
    "Necessary condition is  \n",
    "$$\\frac{\\partial F_\\alpha}{\\partial y} = 0 \\Leftrightarrow 2(\\vw^\\tau\\vw y - \\vx^{\\alpha\\tau}\\vw) = 0$$.\n",
    "\n",
    "$$\\Rightarrow~y^\\alpha = \\frac{\\vw^\\tau\\vx^\\alpha}{\\|\\vw\\|^2}$$ is a latent variable, resp. the projection index for the data point $\\vx^\\alpha$.\n",
    "\n",
    "Inserting $y^\\alpha$ into $E$ gives \n",
    "\\begin{eqnarray}\n",
    "E(\\vw) = \\frac{1}{N}\\sum_\\alpha F_\\alpha(y^\\alpha) &=& \\frac{1}{N}\\sum_\\alpha \\vx^{\\alpha \\tau}\\vx^\\alpha\n",
    "            - \\frac{2}{N}\\sum_\\alpha \\vw^\\tau\\vx^\\alpha\\frac{\\vw^\\tau\\vx^\\alpha}{\\vw^\\tau\\vw}\n",
    "            + \\frac{\\vw^\\tau\\vw}{N}\\sum_\\alpha\\frac{\\vw^\\tau\\vx^\\alpha\\vw^\\tau\\vx^\\alpha}{(\\vw^\\tau\\vw)^2}\\\\\n",
    "        &=& \\frac{1}{N}\\sum_\\alpha \\vx^{\\alpha \\tau}\\vx^\\alpha - \\frac{1}{N}\\vw^\\tau\\sum_\\alpha\n",
    "            \\frac{\\vx^\\alpha\\vx^{\\alpha \\tau}\\vw}{\\|\\vw\\|^2}\n",
    "\\end{eqnarray}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step S2**: Minimize $E$ according to $\\vw$\n",
    "\n",
    "(the first term does not depend on $\\vec{w}$, so only the second remains)\n",
    "\\begin{eqnarray*}\n",
    "\\arg\\min_{\\vw} E(\\vw) &=& \\arg\\max_{\\vw} \\left(\n",
    "    \\frac{\\vw^T}{\\|\\vw\\|}\\left(\n",
    "        \\frac{1}{N}\\sum_\\alpha \\vx^\\alpha\\vx^{\\alpha \\tau}\n",
    "    \\right)\\frac{\\vw}{\\|\\vw\\|}\n",
    "\\right)\\\\\n",
    " &=& \\arg\\max_{\\hat w} \\left(\\hat{w}^\\tau\\mathbf{C}\\hat{w}\\right)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "\n",
    "The axis in direction of the first eigenvector $\\hat{u}_1$ belonging to the largest eigenvalue $\\lambda_1$\n",
    "of the dataset covariance matrix $\\mathbf{C}$ minimizes the mean-square error (MSE).\n",
    "\n",
    "**Remarks:**\n",
    "* Generalization on $q$-dimensional linear manifolds results in principal (hyper-)plains which are spanned by the first $q$ eigenvectors of $\\mathbf{C}$ corresponding to the first $q$ eigenvalues in descending order.\n",
    "* While in 2D (features $x,y$) linear regression minimizes the $y$-distance as depending variable over $x$, here $x$ and $y$ are treated symmetrically, the distance **perpendicular** to the subspace is minimized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.3. Principal Curve and Principal Surface\n",
    "\n",
    "Generalization to nonlinear manifolds are considerably more difficult: depending on the dimention $q$ of the latent variable the result is a principal curve ($q=1$), principal surface ($q=2$), etc.\n",
    "\n",
    "First we look at the special case $q=1$.\n",
    "\n",
    "**Wanted**:\n",
    "\n",
    "Curve $\\vec{f}:\\R\\to\\R^d$, $\\vec{f}(y)$, which minimizes the Mean-Square Error (MSE)\n",
    "$$ E = \\langle \\min_y \\| \\vx - \\vec{f}(y) \\|^2\\rangle$$\n",
    "\n",
    "<img src=\"images/PCurve-english.png\" width=\"60%\">\n",
    "\n",
    "* Problem 1: Parameterization of $\\vec{f}(y)$\n",
    " * polygonal line (i.e. concatenated linear segments)\n",
    " * Polynomial component functions \n",
    "\n",
    "* Problem 2: limitation of flexibility to avoid trivial interpolations\n",
    " * Regularization terms to reward smoothness, e.g.\n",
    " $$ E_G = E + \\int\\left|\\frac{d^2f}{d\\lambda^2}\\right|^2 d\\lambda$$\n",
    " * length constraints\n",
    " \n",
    "* Problem 3: Form of the optimzation landscape: Minimization necessary by all parameters\n",
    " * Nonlinear optimization problem \n",
    " * $\\to$ risk of getting stuck in local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Curve by Hastie and Stuezle (1989)\n",
    "\n",
    "Principal Curve was introduced as a curve which fulfills the so called self-consistency condition\n",
    "$$ \\vec{f}(y) = \\langle \\vx | y_f(\\vx) = y \\rangle_{\\vx} $$\n",
    "\n",
    "* Note that each point of the curve is also the mean of all data points that have this point as their nearest representant.\n",
    "\n",
    "<img src=\"images/PCurveHastie-english.png\" width=\"40%\">\n",
    "\n",
    "This means that each point of the curve is the extremal point of a quadratic distance function\n",
    "$$ E(y^\\star) = \\frac{1}{2}\\int\\limits_{y_f(\\vx) = y^\\star} (\\vx-f(y^\\star))^2  p(\\vx) d\\vx $$\n",
    "* here the integration is over all points that project on $y^\\star$\n",
    "* that this is an optimization problem follows from our first section (principal point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Algorithm**:\n",
    "1. start with $\\vec{f}(y) = \\langle\\vx\\rangle + \\hat u_1 y$ (1st axis of PCA)\n",
    " * i.e. set for each data point $\\vx^\\alpha: ~~~ y^\\alpha = \\hat u_1^\\tau (\\vx^\\alpha - \\bar x)$\n",
    "2. fix $y$ and minimize $\\langle\\| \\vx - \\vec{f}(y)\\|^2\\rangle$ by setting \n",
    "$$ f_j(y) = \\langle x_j | y_f(\\vx) = y \\rangle ~\\forall~ j $$\n",
    " * Note that in case of finite (discrete) data sets we need a neighborhood window so that all data points in the vicinity of $y$ are averaged.\n",
    "A practical choice for that is:\n",
    "$$\n",
    "\\vec{f}(y) = \\frac{\\sum\\limits_\\alpha \\vx^\\alpha \\exp\\left(-\\frac{(y(\\vx^\\alpha)-y)^2}{2\\sigma^2} \\right)}{\\sum\\limits_\\alpha \\exp\\left(-\\frac{(y(\\vx^\\alpha)-y)^2}{2\\sigma^2}\\right)}\n",
    "$$\n",
    "where $\\sigma$ is the width of the neighborhood window. \n",
    "Practically this is not done for all possible $y$, but only for the $y^\\alpha$ on which data points currently project so that the curve is represented by a polygonal line of $N$ samples.\n",
    "3. Hold $\\vec{f}$ fixed and calculate new $y = y_{\\vec{f}}(\\vx^\\alpha)~\\forall~ \\alpha$\n",
    "4. goto 2 as long as the error is reduced by more than a threshold difference  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6.4 Principal Surfaces in general\n",
    "\n",
    "* are the result of a generalization of above principal curve optimization\n",
    "* instead of a linear combination of eigenvectors we regard a parameterized $q$-surface $\\tilde\\vx(y_1,\\dots y_q ; \\vw) \\in \\R^d$.\n",
    "* Determination of parameters $\\vw$ according to an extremality requirement \n",
    "\\begin{eqnarray}\n",
    "\tD[\\vw] &:=& \\frac{1}{2} \\left\\langle\\left(\\vx- \\tilde{\\vx}(y_1 \\dots y_q;\\vw) \\right)^2\\right\\rangle\n",
    "\t\t\\stackrel{!}{=}\\text{minimal}  \\\\\t\n",
    "\t&=& \\frac{1}{2}\\int\\left(\\vx- \\tilde{\\vx}(y_1 \\dots y_q;\\vw) \\right)^2 \n",
    "\tp(\\vx)~d\\vx ~~\\text{for given density} ~~ p(\\vx)\n",
    "\\end{eqnarray}\n",
    "* Optimization gives 'Principal Surfaces' (-curves, -manifolds)\n",
    "* Note: Caution: additional smoothness is to be required, otherwise the manifold will overfit. \n",
    "\n",
    "Smoothness constraints are sometimes implicit in the chosen parameterization of $\\tilde{x}$\n",
    "* e.g. for instance, if a polynom of low order is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Computational Procedure**:\n",
    "\n",
    "* Gradient descent \n",
    "$$\n",
    "\\frac{\\partial D}{\\partial w_j}  = -\\left\\langle\n",
    "\t\\left(\\vx-\\tilde{\\vx}(y_1,\\ldots,y_q; \\vw)\\right)^\\tau \\cdot \n",
    "\t\t\\frac{\\partial}{\\partial w_j} \\tilde{\\vx}(y_1,\\ldots,y_q; \\vw)\\right\\rangle_{p(\\vx)}\n",
    "$$ \n",
    "* In case of finite data sets: \"`stochastic approximation\"'\n",
    "\\begin{eqnarray} \n",
    "\\left.\\frac{\\partial D}{\\partial w_j}\\right|_{\\vx^\\alpha} &\\approx& \n",
    "  \t-\\left(\\vx^\\alpha - \\tilde{\\vx}(y_1^\\alpha,\\ldots, y_q^\\alpha; \\vw)\\right)^\\tau \\cdot \\frac{\\partial}{\\partial w_j}\\tilde{\\vx}(y_1^\\alpha,\\ldots,y_q^\\alpha;\\vw)~~\\text{und damit} \\\\\n",
    "\\Delta\\vw & = & \n",
    "\t-\\eta \\left.\\frac{\\partial D}{\\partial w_j} \\right|_{\\vx^\\alpha} = \n",
    "\t\\eta \\cdot (\\vx^\\alpha - \\tilde{\\vx^\\alpha}(y_1^\\alpha,\\ldots,y_q^\\alpha; \\vw))\\cdot \\frac{\\partial}{\\partial w_j}\\tilde{\\vx}(y_1^\\alpha,\\ldots,y_q^\\alpha;\\vw) \n",
    "\\end{eqnarray}\n",
    "* Attention: $y_i(\\vx)$ depend on $\\vx$ and $\\vw$! (_best-match-point_)\n",
    "\n",
    "**Example:**\n",
    "algorithmic procedure e.g. on a discrete grid A (e.g. $q=2$): $\\tilde{x}(y_1,y_2;\\vw)$.\n",
    "* surface parameters $y_1,y_2$ become discrete grid index points $r \\in A$, \n",
    "* function parameter $\\vw$ become total set of $\\vw=\\{\\vw_{\\vec{r}} | \\vec{r} \\in A\\}$ of lattice point positions $\\vw_r$ in the embedding space $\\R^d$.\n",
    "\n",
    "With this we can represent it in a 'lattice representation':\n",
    "$$\\vx(y_1,y_2;\\vw) \\equiv \\vw_{\\vec{r}} \\in \\R^d $$\n",
    "\n",
    "<img src =\"images/gitter.png\" width=\"30%\">\n",
    "\n",
    "Mean distance between grid and data\n",
    "\n",
    "\\begin{eqnarray}\n",
    "D[\\vw] & = & \n",
    "\t\\frac{1}{2} \\sum\\limits_r \\int\\limits_{F_r} (\\vx-\\vw_{\\vec{r}})^2 p(\\vx)d\\vx \\\\\n",
    "F_{\\vec{r}} & = & \n",
    "\t\\{ \\vx |~ \\|\\vx-\\vw_r\\| \\leq \\|\\vx - \\vw_{r'}\\| ~\\forall~ \\vec{r}' \\not= \\vec{r}\\}\n",
    "\\end{eqnarray}\n",
    "* i.e. 'winner takes all'\n",
    "\n",
    "$F_{\\vec{r}}$ is called _Voronoi cell_ around $r$.\n",
    "\n",
    "\\begin{eqnarray}\n",
    "D^*[\\vw] & = & \n",
    "\t\\frac{1}{2} \\sum\\limits_r \\int\\limits_{F_r}(\\vx-\\vw_r)^2 p(\\vx)d\\vx\\\\\n",
    "\\frac{\\partial D^*}{\\partial \\vw_r} & = & \n",
    "\t-\\int\\limits_{F_r}(\\vx-\\vw_r)\\cdot p(\\vx)d\\vx\\\\\n",
    "\\Delta \\vw_r & = & \n",
    "\t-\\epsilon \\frac{\\partial D^*}{\\partial \\vw_r}   =  \\epsilon \\langle\\vx-\\vw_r \\rangle|_{\\vx \\in F_r}\n",
    "\\end{eqnarray}\n",
    "\n",
    "**Stochastic approximation**: \n",
    "$$\\Delta \\vw_r \\stackrel{def}{=} \\epsilon(\\vx^\\alpha-\\vw_r)$$ where $r$ = index of the center $\\vw_r$ with smallest distance to the actual learning example $\\vx^\\alpha$.\n",
    "\n",
    "* Integration of an additional smoothness constraint / condition for the elimination of the problem of lattice folding:\n",
    "\"`blurring\"' of each adaptation step over the neighborhood of $r$.\n",
    "$$ \\Delta \\vec{w_{r'}} = \\epsilon \\cdot \\underbrace{\\exp\\left(-\\frac{(r-r')^2}{2\\sigma^2}\\right)}_{\\text{blurring function}}(\\vec{x}-\\vec{w_{r'}})~, $$\n",
    "where $r$ is the index of the Voronoi cell in which $\\vec{x}$ falls. \n",
    "* $\\to$ learning rule of Kohonen nets / self-organizing networks\n",
    "* In result we find that the SOM (self-organizing map) algorithms is obtained from an optimization problem (if smoothness constraints are added)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ws18EOT1207]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
