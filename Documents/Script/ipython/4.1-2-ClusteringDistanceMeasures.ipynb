{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "## settings \n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import scipy, scipy.stats\n",
    "from ipywidgets import interact, interactive, fixed\n",
    "import ipywidgets as widgets\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (8.0, 4.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\usepackage{amssymb} \\newcommand{\\R}{\\mathbb{R}} \\newcommand{\\vx}{\\vec{x}} \\newcommand{\\vy}{\\vec{y}} \\newcommand{\\vw}{\\vec{w}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Clustering Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Relevance for Data Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Clusters are the simplest form of a structure\n",
    "* prevalence of clusters suggests relatedness / affinity and presence of correlations.\n",
    "* multidimensional clusters can be interpreted as a simple form of rule\n",
    "* cluster centers offer an economic description of the data (data reduction)\n",
    "\n",
    "Note that the definition of an objective criterion for the discrimination of clusters is difficult."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Distance Measures "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting point: Distance matrix\n",
    "$$\n",
    "D= \\left[ \\begin{array}{cccc}\n",
    "\td_{11} & d_{12} & \\dots & d_{1N} \\\\\n",
    "\t\\vdots & &  & \\vdots \\\\\n",
    "\td_{N1} & \\cdots & \\cdots & d_{NN}\\\\ \n",
    "\t\\end{array} \\right] \n",
    "$$\n",
    "* Note that numbers mean the unsimiliarity (distance) between the corresponding data points\n",
    "* Example: distance between the tastes of different pudding flavours\n",
    "\n",
    "Requirements for a distance measure ($\\forall i,j,k$)\n",
    "\\begin{eqnarray}\n",
    "d_{ij} & =       & d_{ji} \\quad  \\text{symmetry}\\\\\n",
    "d_{ij} & \\geq & 0 \\quad ~~ \\text{positive definite}\\\\\n",
    "d_{ij}+d_{jk} & \\geq & d_{ik} \\quad \\text{triangle inequality}\n",
    "\\end{eqnarray}\n",
    "\n",
    "The definition / derivation of meaningful distances $d_{ij}$ from the data depends on the fundamental question of the meaning (semantics) of the data:\n",
    "* stating two data points $x_i$ and $x_j$ as similar (i.e. a small value of $d_{ij}$) requires a decision about what features seem to be meaningful.\n",
    "* for that reason, there is no general procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Distance measures for data points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance measure for real valued data vectors are\n",
    "\n",
    "#### 1. Euclidean Distance \n",
    "$$\n",
    "d(\\vx,\\vy)= \\|\\vx-\\vy\\| = \\sqrt{\\sum\\limits_{i=1}^d (x_i-y_i)^2} \n",
    "$$\n",
    "* most simple, straightforward, and frequently used distance meassure\n",
    "* but often insufficient as the following example illustrates\n",
    "* Example: data set of broomstick features:\t\n",
    " * $\\vx = (\\mbox{length in cm}, \\text{diameter of the stick in cm})$\n",
    " * typical broomsticks {(150, 2.0), (158, 2.1), (165, 2.5), (180, 2.4), (170, 2.2)}\n",
    " * a difference of 10 cm in diameter is semantically much more 'different' than the equal difference in length\n",
    " * however, the euclidean distance of (160, 2.0) and (150, 12.0) to (150, 2.0) is the same dissimiliarity! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pearson- or $\\chi^2$ distance\n",
    "\n",
    "$$\td(\\vx,\\vy)= \\sqrt{\\sum\\limits_i \\frac{(x_i-y_i)^2}{\\sigma_i^2}} $$\n",
    "with \n",
    "$\\sigma_i^2 = \\langle(x_i- \\langle x \\rangle)^2\\rangle_x$ as scaling factor  \n",
    "\n",
    "* (+) leads to a more balanced weighting of different dimensions for the result\n",
    "* (-) Pearson-distance assumes uncorrelated vector components $x_i$. This presumption is often not valid\n",
    " * Example: repeated features within the vector, e.g. with different units, or basically a 1:1-dependency\n",
    "\t$$ \\vx=\\left( \n",
    "\t\t\\begin{array}{c} u\\\\ \\vdots \\\\ u \\\\ v \\end{array} \n",
    "\t\t\\right) \\left. \\begin{array}[t]{p{0cm}} \\\\ \\\\  \\end{array} \\right\\} (n-1)-\\mbox{times} \n",
    "\t$$\n",
    "* Iso-Distance-surface to a vector $\\vx$ is an ellipsoid whose principal axes are aligned with the coordinates.\n",
    " * With reference to this, an improved distance measure is "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Mahalanobis-Distance:\n",
    "$$\n",
    "d(\\vx,\\vy)= \\sqrt{(\\vx-\\vy)^\\tau \\Sigma^{-1} (\\vx-\\vy)}\n",
    "$$\n",
    "with \n",
    "$$\n",
    "\t\\Sigma = \\langle(\\vx-\\langle \\vx \\rangle)(\\vx-\\langle \\vx \\rangle)^\\tau \\rangle \n",
    "$$\n",
    "* This basically scales as pearson, but with prior change into the PCA basis\n",
    "* Iso-distance surface around $\\vx$ are now rotated ellipsoids (according to the variance ellipsoid of the whole data set)\n",
    "* Example: \n",
    " * w.l.o.g. let $\\vy = 0$. \n",
    " * Let's look at the vector of the $i$th principal component of length  $\\sqrt{\\lambda_i}$, so $\\vx = \\sqrt{\\lambda_i}\\hat{u}_i$. \n",
    " * Then the distance $d$\n",
    "$$\n",
    "d = \\sqrt{\\lambda_i} \\hat{u}_i^{\\tau} [U D^{-1} U^{\\tau}] \\sqrt{\\lambda_i}\\hat{u}_i \n",
    "= \\sqrt{\\lambda_i} \\lambda_i^{-1} \\sqrt{\\lambda_i} = 1 \n",
    "$$\n",
    " * so the iso-distance surface for the distance 1 has an extension of $\\lambda_i$ along the eigenvector $\\hat{u}_i$\n",
    "* Attention: Scaling can sometimes even destroy a prevalent clustering structure "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. `City Block'-Distance\n",
    "$$\n",
    "d(\\vx,\\vy)= \\sum\\limits_{i=1}^d |y_i-x_i| \n",
    "$$\n",
    "* 'distance to be traveled if streets and avenues are perpendicular, as in Manhattan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Supremum Distance\n",
    "$$\n",
    "d(\\vx,\\vy)= \\max_{i=1 \\dots d} |y_i-x_i| \n",
    "$$\n",
    "* that is the largest component of the city-block sum terms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Minkowski-Distance \n",
    "$$\n",
    "d(x,y)= \\left\\{ \\sum\\limits_{i=1}^d |y_i-x_i|^p \\right\\}^{1/p} \n",
    "$$\n",
    "Some of the above distance measures result as special case, namely:\n",
    "* $p=1$:  City-Block distance\n",
    "* $p=2$:  Euklidean distance \n",
    "* $p\\to \\infty$:  Supremum distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarks:**\n",
    "* All above distance measures assume implicityl a topology of $\\R^n$\n",
    "* They are not suitable to represent angles (which have a topology of a circle)\n",
    "* Different topologies can be tackled by embedding into a suitable $\\R^m$\n",
    "* Example: \n",
    " * $\\phi_1 = 0$ and $\\phi_2 = 2\\pi$ represent the same angle, but have a numeric distance of $2\\pi$.\n",
    " * Embedding the angle variable into a 2D-space by $(\\cos(\\phi), \\sin(\\phi))$ gives a representation where this problem does not occur anymore.\n",
    "* Dealing with nominal attributes\n",
    " * if a fixed number (e.g. $K$) of alternative values are given, the variable can be embedded into a $K$-dimensional vector space, e.g. \n",
    "$$\n",
    "\t\\{\\mbox{vanille, chocolade, strawberry}\\}\\Rightarrow \\left\\{ \\left( \\begin{array}{c}1\\\\0\\\\0\\end{array}\n",
    "\t\\right),\\left( \\begin{array}{c}0\\\\1\\\\0\\end{array} \\right),\\left( \\begin{array}{c}0\\\\0\\\\1\\end{array} \\right)\\right\\} \n",
    "$$\n",
    " * Using a numbering $\\{V, C, S\\} \\Rightarrow \\{1,2,3\\}$ would instead induce an ordering (e.g. $d(V,C)<d(V, S)$)\n",
    "\n",
    " * Trick: if many value alternatives are given, the embedding space would become inadequately high-dimensional. A sometimes acceptable compromise is then to use random projections: select $K$ random vectors of length 1 in a vector space $\\R^L,~L < K$. If $L$ is large enough, the vectors are approximately orthogonal on eachother and therefore more or less uncorrelated.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2. Distance measures between clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many clustering methods require a distance between clusters.\n",
    "Let $X, Y$ be clusters, we can define the following frequently used distance measures \n",
    "\n",
    "\\begin{eqnarray}\n",
    "d_1(X,Y) =  \\min\\limits_{\\vx \\in X \\atop \\vy \\in Y} d(\\vx,\\vy) && \\mbox{minimal distance} \\\\[4mm]\n",
    "d_2(X,Y) =  \\max\\limits_{\\vx \\in X \\atop \\vy \\in Y} d(\\vx,\\vy) && \\mbox{maximal distance} \\\\[4mm]\n",
    "d_3(X,Y) =  \\frac{1}{N_X N_Y} \\sum\\limits_{\\vx \\in X \\atop \\vy \\in Y} d(\\vx,\\vy) && \\mbox{average distance}\\\\[4mm]\n",
    "d_4(X,Y) =  d\\left(\\frac{1}{N_X} \\sum\\limits_{\\vx \\in X}\\vx,\n",
    "\t \\frac{1}{N_Y} \\sum\\limits_{\\vy \\in Y}\\vy \\right) && \\mbox{centroid distance}\n",
    "\\end{eqnarray}\n",
    "\n",
    "<img src=\"images/cluster-distances.png\" width=\"50%\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
